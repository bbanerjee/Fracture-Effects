\chapter{The MPM component}
The \MPM component solves the momentum equations
\Beq
  \Div{\Bsig} + \rho\Bb = \rho\dot{\Bv} 
\Eeq
using an updated Lagrangian formulation. The momentum solve for solid materials is
complicated by the fact that the equations need material constitutive models for
closure.  These material constitutive models vary significantly between materials
and contribute a large fraction of the computational cost of a simulation.

In this chapter we discuss the algorithm used in \Vaango to solve the momentum
equations using \MPM.  The implementation follows the approach discussed in
the previous chapter.

\section{The MPM algorithm}

The momentum equation is solved using the MPM algorithm while forward Euler time-stepping
is use to integrate time derivatives.  The pseudocode of the overall algorithm is given below.
The main quantities of interest are:
\begin{itemize} 
  \setlength\itemsep{1pt}
  \item $t_\Tmax$ : {\Ochre The maximum time until which the simulation is to run.}
  \item $t, \Delta t$ : {\Ochre The current time ($t = t_n$) and the time step.}
  \item $\Bh_g$ : {\Ochre The grid spacing vector.}
  \item $m_p$ : {\Ochre The particle mass.}
  \item $V_p^n, V_p^{n+1}$ : {\Ochre The particle volume at $t = t_n$ and $t = t_{n+1}$.}
  \item $\Bx_p^n, \Bx_p^{n+1}$ : {\Ochre The particle position at $t = t_n$ and $t = t_{n+1}$.}
  \item $\Bu_p^n, \Bu_p^{n+1}$ : {\Ochre The particle displacement at $t = t_n$ and $t = t_{n+1}$.}
  \item $\Bv_p^n, \Bv_p^{n+1}$ : {\Ochre The particle velocity at $t = t_n$ and $t = t_{n+1}$.}
  \item $\Bsig_p^n, \Bsig_p^{n+1}$ : {\Ochre The particle Cauchy stress at time $t = t_n$ and $t = t_{n+1}$.}
  \item $\BF_p^n, \BF_p^{n+1}$ : {\Ochre The particle deformation gradient at time $t = t_n$ and $t = t_{n+1}$.}
\end{itemize}

\begin{breakablealgorithm}
  \caption{The algorithm}
  \begin{algorithmic}[1]
    \Procedure{run}{inputUPSFile}
      \State $t_\Tmax$, $\Bh_g$, \TTObj{xmlProblemSpec}, \TTObj{grid}, \TTObj{globalState} $\leftarrow$ 
             \textsc{readInputUPSFile}(inputUPSFile)
        \Comment{Parse \WRP the input XML file (<filename>.ups), create the background grid,
                 and \WRP set up a \textsc{SimulationState}.}
      \State \TTObj{mpmFlags}, \TTObj{prescribedDefGrad}, \TTObj{particleBC}, \TTObj{contactModel}, 
             \TTObj{constitutiveModel}, \WRP \TTObj{defGradComputer}, \TTObj{damageModel} $\leftarrow$
              \textsc{problemSetup}(\TTObj{xmlProblemSpec}, \TTObj{grid}, \WRP \TTObj{globalState})
        \Comment{Set up flags, the constitutive model, and the 
                 deformation gradient \WRP algorithm based on data in input file.}
      \State $t \leftarrow 0$, $n \leftarrow 0$
      \State $\Bx^n_p$, $\Bu^n_p$, $m_p$, $V^n_p$, $\Bv^n_p$, $\Bsig^n_p$, $\BF^n_p$ $\leftarrow$
               \textsc{initialize}(\TTObj{xmlProblemSpec})
        \Comment{Find the grid size and initialize \WRP particle variables based on geometry and other
                 information in the input file.}
      \State isSuccess $\leftarrow$ FALSE
      \Repeat
        \State $\Delta t \leftarrow$ \textsc{computeStableTimeStep}($\Bh_g$, $\Bv_p$)
          \Comment{Find a stable time increment based on \WRP grid size and velocity}
        \State $t \leftarrow t + \Delta t$, $n \leftarrow n+1$
          \Comment{Update the time}
        \State isSuccess, $\Bx^{n+1}_p$, $\Bu^{n+1}_p$, $v^{n+1}_p$, $\Bv^{n+1}_p$, $\Bsig^{n+1}_p$, $\BF^{n+1}_p$ 
           $\leftarrow$ \textsc{timeAdvance}($\Bh_g$, $\Bx^n_p$, $\Bu^n_p$, $m_p$, $V^n_p$, \WRP $\Bv^n_p$, $\Bsig^n_p$, $\BF^n_p$)
           \Comment{Compute updated quantities}
        \State \textsc{outputData}($\Bx^{n+1}_p$, $\Bu^{n+1}_p$, $V^{n+1}_p$, $\Bv^{n+1}_p$, $\Bsig^{n+1}_p$, $\BF^{n+1}_p$) 
           \Comment{Save the solution}
        \State $n \leftarrow n+1$
      \Until {$t \ge t_\Tmax$}
      \State \Return isSuccess
    \EndProcedure
  \end{algorithmic}
\end{breakablealgorithm}
      
\section{Reading the input file}
The process used to read the input file is identical to that discussed earlier in this manual.

\section{Problem setup}
The overall structure of the problem setup code is given below.  Details can be found in
the code.
\begin{breakablealgorithm}
  \caption{Problem setup}
  \begin{algorithmic}[1]
    \Require \TTObj{xmlProblemSpec}, \TTObj{grid}, \TTObj{globalState}
    \Procedure{problemSetup}{\TTObj{xmlProblemSpec}, \TTObj{grid}, \TTObj{globalState}}
      \State \TTObj{flags} $\leftarrow$ \textsc{readMPMFlags}(\TTObj{xmlProblemSpec})
         \Comment{Read the option flags that determine \WRP the details of the MPM algorithm to be
                  used in the simulation.}
      \If {\TTObj{flags}.prescribeDeformation = TRUE}
         \State \TTObj{prescribedDefGrad} $\leftarrow$ \textsc{readPrescribedDeformations}(\TTObj{flags}.prescribedFileName)
      \EndIf
      \State \TTObj{particleBC} $\leftarrow$ \textsc{createMPMPhysicalBC}(\TTObj{xmlProblemSpec}, \TTObj{grid}, \TTObj{flags})
        \Comment{Create the model\WRP  used to apply pressures and forces directly to particles.}
      \State \TTObj{contactModel} $\leftarrow$ \textsc{createContactModel}(\TTObj{xmlProblemSpec}, \TTObj{grid}, \TTObj{flags}, \TTObj{globalState})
        \Comment{Create\WRP  the contact algorithm model used to compute interactions between
                 objects.}
      \State \TTObj{constitutiveModel} $\leftarrow$ \textsc{createConstitutiveModels}(\TTObj{xmlProblemSpec}, \TTObj{grid}, \TTObj{flags}, \WRP \TTObj{globalState})
        \Comment{Create the constitutive models that are needed\WRP  for the simulation.}
      \State \TTObj{defGradComputer} $\leftarrow$ \textsc{createDeformationGradientComputer}(\TTObj{flags}, \TTObj{globalState})
        \Comment{Create \WRP the model that will be used to compute velocity and \WRP deformation
                 gradients.}
      \If {\TTObj{flags}.doBasicDamage = TRUE}
        \State \TTObj{damageModel} $\leftarrow$ \textsc{createBasicDamageModel}(\TTObj{flags}, \TTObj{globalState})
      \EndIf
      \State \Return \TTObj{flags}, \TTObj{prescribedDefGrad}, \TTObj{particleBC}, 
         \TTObj{contactModel}, \TTObj{constitutiveModel}, \WRP
         \TTObj{damageModel}, \TTObj{defGradComputer}
    \EndProcedure
  \end{algorithmic}
\end{breakablealgorithm}

\section{Initialization}
An outline of the initialization process is described below.  Specific details have been 
discussed in earlier reports. The new quantities introduced in this section are
\begin{itemize} 
  \setlength\itemsep{1pt}
  \item $n_p$ : {\Ochre The number of particles used to discretize a body.}
  \item $\Bb_p^n, \Bb_p^{n+1}$ : {\Ochre The particle body force acceleration at $t = t_n$ and $t = t_{n+1}$.}
  \item $D_p^n, D_p^{n+1}$ : {\Ochre The particle damage parameter at $t = t_n$ and $t = t_{n+1}$.}
  \item $\Bf_p^{\Text,n}, \Bf_p^{\Text,n+1}$ : {\Ochre The particle external force at $t = t_n$ and $t = t_{n+1}$.}
\end{itemize}
\begin{breakablealgorithm}
  \caption{Initialization}
  \begin{algorithmic}[1]
    \Require \TTObj{xmlProblemSpec}, \TTObj{defGradComputer}, \TTObj{constitutiveModel}, 
             \TTObj{damageModel}, \TTObj{particleBC},\WRP 
             \TTObj{mpmFlags} \TTList{materialList}, 
    \Procedure{initialize}{}
      \For{\TTmatl\, \textbf{in} \TTList{materialList}}
        \State $n_p$[\TTmatl], $\Bx^0_p$[\TTmatl], $\Bu^0_p$[\TTmatl], $m_p$[\TTmatl], 
             $V^0_p$[\TTmatl], $\Bv^0_p$[\TTmatl], $\Bb^0_p$[\TTmatl], \WRP
             $\Bf^{\Text,0}_p$[\TTmatl] $\leftarrow$ \TTmatl.\textsc{createParticles}()
        \State $\BF^0_p$[\TTmatl] $\leftarrow$ \TTObj{defGradComputer}.\textsc{initialize}(\TTmatl)
        \State $\Bsig^0_p$[\TTmatl] $\leftarrow$ \TTObj{constitutiveModel}.\textsc{initialize}(\TTmatl)
        \State $D^0_p$[\TTmatl] $\leftarrow$ \TTObj{damageModel}.\textsc{initialize}(\TTmatl)
      \EndFor
      \If {\TTObj{mpmFlags}.initializeStressWithBodyForce = TRUE}
        \State $\Bb^0_p \leftarrow$ \textsc{initializeBodyForce}()
        \State $\Bsig^0_p, \BF^0_p$ $\leftarrow$ \textsc{initializeStressAndDefGradFromBodyForce}()
      \EndIf
      \If {\TTObj{mpmFlags}.applyParticleBCs = TRUE}
        \State $\Bf^{\Text,0}_p$ $\leftarrow$ \TTObj{particleBC}.\textsc{initializePressureBCs}()
      \EndIf
      \State \Return $n_p$, $\Bx^0_p$, $\Bu^0_p$, $m_p$, 
             $V^0_p$, $\Bv^0_p$, $\Bb^0_p$, $\Bf^{\Text,0}_p$, $\BF^0_p$, $\Bsig^0_p$, $D^0_p$
    \EndProcedure
  \end{algorithmic}
\end{breakablealgorithm}

\section{Time advance}
The operations performed during a timestep are shown in the pseudocode below.
\begin{breakablealgorithm}
  \caption{The MPM time advance algorithm}
  \begin{algorithmic}[1]
    \Procedure{timeAdvance}{$\Bh_g$, $x^n_p$, $u^n_p$, $m_p$, $V^n_p$, $\Bv^n_p$, $\Bf^{\Text,n}_p$,
                            $\BdT^n_p$}%, $\Bsig^n_p$}%, $\BF^n_p$}
      \State $\Bb_p^n \leftarrow$ \textsc{computeParticleBodyForce}()
        \Comment{Compute the body force term}
      \State $\Bf_p^{\Text,n+1} \leftarrow$ \textsc{applyExternalLoads}()
        \Comment{Apply external loads to the particles}
      \State $m_g$, $V_g$, $\Bv_g$, $\Bb_g$, $\Bf^\Text_g$ $\leftarrow $ 
        \textsc{interpolateParticlesToGrid}()
        \Comment{Interpolate particle data to the grid}
      \State \textsc{exchangeMomentumInterpolated}()
        \Comment{Exchange momentum between bodies on grid. \WRP Not discussed in this report.}
      \State $\Bf^\Tint_g$, $\Bsig_g$, $\Bv_g$ $\leftarrow$ \textsc{computeInternalForce}()
        \Comment{Compute the internal force at the grid nodes}
      \State $\Bv_g^\star$, $\Ba_g$ $\leftarrow$ \textsc{computeAndIntegrateAcceleration}()
        \Comment{Compute the grid velocity \WRP and grid acceleration}
      \State \textsc{exchangeMomentumIntegrated}()
        \Comment{Exchange momentum between bodies on grid \WRP using integrated values.  
                 Not discussed in this report.}
      \State $\Bv_g^\star$, $\Ba_g$ $\leftarrow$ \textsc{setGridBoundaryConditions}()
        \Comment{Update the grid velocity and grid \WRP acceleration using the BCs}
      \State $\BlT_p^n$, $\BF_p^{n+1}$, $V_p^{n+1}$ $\leftarrow$ \textsc{computeDeformationGradient}()
        \Comment{Compute the velocity gradient \WRP and the deformation gradient}
      \State $\Bsig_p^{n+1}$, $\Beta_p^{n+1}$ $\leftarrow$ \textsc{computeStressTensor}()
        \Comment{Compute the updated stress and \WRP internal variables (if any)}
      \State $\Bsig_p^{n+1}$, $\Beta_p^{n+1}$, $\chi_p^{n+1}$, $D_p^{n+1}$ $\leftarrow$ 
        \textsc{computeBasicDamage}()
        \Comment{Compute the damage parameter \WRP and update the stress and internal variables}
      \State $\chi_p^{n+1}$, $D_p^{n+1}$ $\leftarrow$ \textsc{updateErosionParameter}()
        \Comment{Update the indicator variable that is used \WRP to delete particles at the
                 end of a time step}
      \State $V_p^{n+1}$, $\Bu_p^{n+1}$, $\Bv_p^{n+1}$, $\Bx_p^{n+1}$, $m_p$, $\Bh_p^{n+1}$ $\leftarrow$
        \textsc{interpolateToParticlesAndUpdate}()
        \Comment{Update the \WRP particle variables after interpolating grid quantities to particles}
    \EndProcedure
  \end{algorithmic}
\end{breakablealgorithm}
The algorithms used for the above operations are discussed next.

\subsection{Computing the body force}
The body force consists of a gravitational term and, optionally, centrifugal and coriolis terms
that are needed for simulations inside a rotating frame such as a centrifuge.
\begin{breakablealgorithm}
  \caption{Computing the body force on particles}
  \begin{algorithmic}[1]
    \Require $\Bx_p^n$, $\Bv_p^n$, \TTList{materialList}, \TTList{particleList}, \TTObj{mpmFlags}
    \Procedure{computeParticleBodyForce}{}
      \For{\TTmatl\, \textbf{in} \TTList{materialList}}
        \If {\TTObj{mpmFlags}.\texttt{rotatingCoordSystem} = TRUE}
           \State $\Bg \leftarrow$ \TTObj{mpmFlags}.\texttt{gravityAcceleration}
           \State $\Bb_p^{n}$[\TTmatl] $\leftarrow$ $\Bg$
        \Else
          \For{\TTpart\, \textbf{in} \TTList{particleList}}
             \State $\Bg \leftarrow$ \TTObj{mpmFlags}.\texttt{gravityAcceleration}
             \State $\Bx_{rc} \leftarrow$ \TTObj{mpmFlags}.\texttt{coordRotationCenter}
             \State $\Bz_r \leftarrow$ \TTObj{mpmFlags}.\texttt{coordRotationAxis}
             \State $w \leftarrow$ \TTObj{mpmFlags}.\texttt{coordRotationSpeed}
             \State $\Bomega \leftarrow w\Bz_r$
               \Comment{Compute angular velocity vector}
             \State $\Ba_{\text{corolis}} \leftarrow 2 \Bomega \times \Bv_p^{n}[\TTmatl,\TTpart]$
               \Comment{Compute Coriolis acceleration}
             \State $\Br \leftarrow \Bx_p^{n}[\TTmatl,\TTpart] - \Bx_{rc}$
             \State $\Ba_{\text{centrifugal}} \leftarrow \Bomega \times \Bomega \times \Br$
               \Comment{Compute the centrifugal body force acceleration}
             \State $\Bb_p^{n}$[\TTmatl,\TTpart] $\leftarrow$ 
                $\Bg -\Ba_{\text{centrifugal}} - \Ba_{\text{coriolis}}$
               \Comment{Compute the body force acceleration}
          \EndFor
        \EndIf
      \EndFor
      \State \Return $\Bb_p^{n}$
    \EndProcedure
  \end{algorithmic}
\end{breakablealgorithm}

\subsection{Applying external loads}
Note that the updated deformation gradient has not been computed yet at this stage and the 
particle force is applied based on the deformation gradient at the beginning of the timestep.
The new quantities introduced in this section are:
\begin{itemize} 
  \setlength\itemsep{1pt}
  \item $\Bh_p^n$ : {\Ochre The particle size matrix at time $t = t_n$.}
\end{itemize}
\begin{breakablealgorithm}
  \caption{Applying external loads to particles}
  \begin{algorithmic}[1]
    \Require $t_{n+1}$, $\Bx_p^n$, $\Bh_p^n$, $\Bu_p^n$, $\Bf^{\Text,n}_p$, $\BF_p^n$,
             \TTList{materialList}, \TTList{particleList}, 
             \TTObj{mpmFlags}, \TTObj{particleBC}
    \Procedure{applyExternalLoads}{}
      \State $f_p$ $\leftarrow$ $0$
      \If {\TTObj{mpmFlags}.\texttt{useLoadCurves} = TRUE}
        \State $f_p$ $\leftarrow$ \TTObj{particleBC}.\textsc{computeForcePerParticle}($t^{n+1}$)
        \Comment{Compute the force per particle \WRP due to the applied pressure}
      \EndIf
      \For{\TTmatl\, \textbf{in} \TTList{materialList}}
        \If {\TTObj{mpmFlags}.\texttt{useLoadCurves} = TRUE}
          \For{\TTpart\, \textbf{in} \TTList{particleList}}
            \State $\Bf^{\Text,n+1}_p$[\TTmatl,\TTpart] $\leftarrow$ 
               \TTObj{particleBC}.\textsc{getForceVector}($t_{n+1}$, $\Bx_p^n$, $\Bh_p^n$, $\Bu_p^n$, \WWRP
                  $f_p$, $\BF_p^{n}$)
               \Comment{Compute the applied force vector at each particle}
          \EndFor
        \Else
          \State $\Bf^{\Text,n+1}_p$[\TTmatl] $\leftarrow$ $\Bf^{\Text,n}_p$[\TTmatl]
        \EndIf
      \EndFor
      \State \Return $\Bf_p^{\Text,n+1}$
    \EndProcedure
  \end{algorithmic}
\end{breakablealgorithm}

\subsection{Interpolating particles to grid}
The grid quantities computed during this procedure and not stored for the next timestep except for
the purpose of visualization.  The new quantities introduced in this section are
\begin{itemize} 
  \setlength\itemsep{1pt}
  \item $m_g$ : {\Ochre The mass at a grid node.}
  \item $V_g$ : {\Ochre The volume at a grid node.}
  \item $\Bv_g$ : {\Ochre The velocity at a grid node.}
  \item $\Bf^\Text_g$ : {\Ochre The external force at a grid node.}
  \item $\Bb_g$ : {\Ochre The body force at a grid node.}
\end{itemize}
\begin{breakablealgorithm}
  \caption{Interpolating particle data to background grid}
  \begin{algorithmic}[1]
    \Require $m_p$, $V_p^n$, $\Bx_p^n$, $\Bh_p^n$, $\Bb_p^n$, $\Bf^{\Text,n+1}_p$, $\BF_p^n$,
             \TTList{materialList}, \TTList{particleList}, \TTList{gridNodeList}
             \TTObj{mpmFlags}, \TTObj{particleBC}
    \Procedure{interpolateParticlesToGrid}{}
      \State \TTObj{interpolator} $\leftarrow$ \textsc{createInterpolator}(\TTObj{mpmFlags})
        \Comment{Create the interpolator \WRP and find number of grid nodes that can affect a particle}
      \For{\TTmatl\, \textbf{in} \TTList{materialList}}
        \For{\TTpart\, \textbf{in} \TTList{particleList}}
          \State $n_{gp}$, $S_{gp}$ $\leftarrow$ 
            \TTObj{interpolator}.\textsc{findCellsAndWeights}($\Bx_p^n$, $\Bh_p^n$, $\BF_p^n$)
            \Comment{Find the node \WWRP indices of the cells affecting the particle and the 
                     interpolation weights}
          \State $\Bp_p$ $\leftarrow$ $m_p$[\TTmatl][\TTpart] $\Bv_p^n$[\TTmatl][\TTpart] 
            \Comment{Compute particle momentum}
          \For{\TTnode\, \textbf{in} $n_{gp}$}
            \State $m_g$[\TTmatl][\TTnode] $\leftarrow$ $m_g$[\TTmatl][\TTnode] + $m_p$[\TTmatl][\TTpart] $S_{gp}$[\TTnode]
            \State $V_g$[\TTmatl][\TTnode] $\leftarrow$ $V_g$[\TTmatl][\TTnode] + $V_p^n$[\TTmatl][\TTpart] $S_{gp}$[\TTnode]
            \State $\Bv_g$[\TTmatl][\TTnode] $\leftarrow$ $\Bv_g$[\TTmatl][\TTnode] + $\Bp_p$ $S_{gp}$[\TTnode]
            \State $\Bf^{\Text}_g$[\TTmatl][\TTnode] $\leftarrow$ $\Bf^{\Text}_g$[\TTmatl][\TTnode] + $\Bf^{\Text,n+1}_p$[\TTmatl][\TTpart] $S_{gp}$[\TTnode]
            \State $\Bb_g$[\TTnode] $\leftarrow$ $\Bb_g$[\TTnode] + $m_p$[\TTmatl][\TTpart] $\Bb^n_p$[\TTmatl][\TTpart] $S_{gp}$[\TTnode]
          \EndFor
        \EndFor
        \For{\TTnode\, \textbf{in} \TTList{gridNodeList}}
          \State $\Bv_g$[\TTmatl][\TTnode] $\leftarrow$ $\Bv_g$[\TTmatl][\TTnode]/$m_g$[\TTmatl][\TTnode]
        \EndFor
        \State $\Bv_g$[\TTmatl] $\leftarrow$ \textsc{applySymmetryVelocityBC}($\Bv_g$[\TTmatl])
          \Comment{Apply any symmetry \WRP velocity BCs that may be applicable}
      \EndFor
      \State \Return $m_g$, $V_g$, $\Bv_g$, $\Bb_g$, $\Bf_g^{\Text}$
    \EndProcedure
  \end{algorithmic}
\end{breakablealgorithm}

\subsection{Exchanging momentum using interpolated grid values}
The exchange of momentum is carried out using a contact model.  Details can be found in the
\Vaango Developers Manual.

\subsection{Computing the internal force}
This procedure computes the internal force at the grid nodes. The new quantities introduced in this 
section are
\begin{itemize} 
  \setlength\itemsep{1pt}
  \item $n_{gp}$ : {\Ochre The number of grid nodes that are used to interpolate from particle to grid.}
  \item $S_{gp}$ : {\Ochre The nodal interpolation function evaluated at a particle}
  \item $\BG_{gp}$ : {\Ochre The gradient of the nodal interpolation function evaluated at a particle}
  \item $\Bsig_v$ : {\Ochre A volume weighted grid node stress.}
  \item $\Bf^\Tint_g$ : {\Ochre The internal force at a grid node.}
\end{itemize}
\begin{breakablealgorithm}
  \caption{Computing the internal force}
  \begin{algorithmic}[1]
    \Require $\Bh_g$, $V_g$, $V_p^n$, $\Bx_p^n$, $\Bh_p^n$, $\Bsig_p^n$, $\BF_p^n$,
             \TTList{materialList}, \TTList{particleList}, \TTList{gridNodeList}
             \TTObj{mpmFlags}
    \Procedure{computeInternalForce}{}
      \State \TTObj{interpolator} $\leftarrow$ \textsc{createInterpolator}(\TTObj{mpmFlags})
        \Comment{Create the interpolator and \WRP find number of grid nodes that can affect a particle}
      \For{\TTmatl\, \textbf{in} \TTList{materialList}}
        \For{\TTpart\, \textbf{in} \TTList{particleList}}
          \State $n_{gp}$, $S_{gp}$, $\BGv_{gp}$ $\leftarrow$   \WWRP
            \TTObj{interpolator}.\textsc{findCellsAndWeightsAndShapeDervatives}($\Bx_p^n$, $\Bh_p^n$, $\BF_p^n$) \WWRP
            \Comment{Find the node indices of the cells affecting the particle and \WWRP the 
                     interpolation weights and gradients}
          \State $\Bsig_v$ $\leftarrow$ $V_p$[\TTmatl][\TTpart] $\Bsig_p^n$[\TTmatl][\TTpart] 
          \For{\TTnode\, \textbf{in} $n_{gp}$}
            \State $\Bf^{\Tint}_g$[\TTmatl][\TTnode] $\leftarrow$ $\Bf^{\Tint}_g$[\TTmatl][\TTnode] - 
               ($\BGv_{gp}$[\TTnode]$/\Bh_g$) $\cdot \Bsig_p^n$[\TTmatl][\TTpart] $V_p^n$[\TTpart]
            \State $\Bsig_g$[\TTmatl][\TTnode] $\leftarrow$ $\Bsig_g$[\TTmatl][\TTnode] + $\Bsig_v$ $S_{gp}$[\TTnode]
          \EndFor
        \EndFor
        \For{\TTnode\, \textbf{in} \TTList{gridNodeList}}
          \State $\Bsig_g$[\TTmatl][\TTnode] $\leftarrow$ $\Bsig_g$[\TTmatl][\TTnode]/$V_g$[\TTmatl][\TTnode]
        \EndFor
        \State $\Bv_g$[\TTmatl] $\leftarrow$ \textsc{applySymmetryTractionBC}()
          \Comment{Apply any symmetry tractions BCs \WWRP that may be applicable}
      \EndFor
      \State \Return $\Bf_g^{\Tint}$, $\Bsig_g$, $\Bv_g$
    \EndProcedure
  \end{algorithmic}
\end{breakablealgorithm}

\subsection{Computing and integrating the acceleration}
This procedure computes the accelerations at the grid nodes and integrates the grid accelerations
using forward Euler to compute grid velocities. The new quantities introduced in this section are
\begin{itemize} 
  \setlength\itemsep{1pt}
  \item $\Ba_{g}$ : {\Ochre The grid accelerations.}
  \item $\Bv_{g}^\star$ : {\Ochre The integrated grid velocities.}
\end{itemize}
\begin{breakablealgorithm}
  \caption{Computing and integrating the acceleration}
  \begin{algorithmic}[1]
    \Require $\Delta t$, $m_g$, $\Bf^{\Tint}_g$, $\Bf^{\Text}_g$, $\Bb_g$, $\Bv_g$,
             \TTList{materialList}, \TTList{gridNodeList},
             \TTObj{mpmFlags}
    \Procedure{computeAndIntegrateAcceleration}{}
      \For{\TTmatl\, \textbf{in} \TTList{materialList}}
        \For{\TTnode\, \textbf{in} \TTList{gridNodeList}}
          \State $\Ba_g$[\TTmatl][\TTnode] $\leftarrow$ 
            ($\Bf^{\Tint}_g$[\TTmatl][\TTnode] +
             $\Bf^{\Text}_g$[\TTmatl][\TTnode] + 
             $\Bb_g$[\TTmatl][\TTnode])$/m_g$[\TTmatl][\TTnode]
          \State $\Bv_g^{\star}$ $\leftarrow$ $\Bv_g$[\TTmatl][\TTnode] + 
            $\Ba_g$[\TTmatl][\TTnode] $* \Delta t$
        \EndFor
      \EndFor
      \State \Return $\Bv_g^{\star}$, $\Ba_g$
    \EndProcedure
  \end{algorithmic}
\end{breakablealgorithm}

\subsection{Exchanging momentum using integrated grid values}
The exchange of momentum is carried out using a contact model.  Details can be found in the
\Vaango Developers Manual.

\subsection{Setting grid boundary conditions}
\begin{breakablealgorithm}
  \caption{Setting grid boundary conditions}
  \begin{algorithmic}[1]
    \Require $\Delta t$, $\Ba_g$, $\Bv_g^{\star}$, $\Bv_g$,
             \TTList{materialList}, \TTList{gridNodeList},
             \TTObj{mpmFlags}
    \Procedure{setGridBoundaryConditions}{}
      \For{\TTmatl\, \textbf{in} \TTList{materialList}}
        \State $\Bv_g^\star$[\TTmatl] $\leftarrow$ \textsc{applySymmetryVelocityBC}($\Bv_g^\star$[\TTmatl])
        \For{\TTnode\, \textbf{in} \TTList{gridNodeList}}
          \State $\Ba_g$[\TTmatl][\TTnode] $\leftarrow$ ($\Bv_g^\star$[\TTmatl][\TTnode] - $\Bv_g$[\TTmatl][\TTnode]) $/ \Delta t$
        \EndFor
      \EndFor
      \State \Return $\Bv_g^\star$, $\Ba_g$
    \EndProcedure
  \end{algorithmic}
\end{breakablealgorithm}

\subsection{Computing the deformation gradient}
The velocity gradient is computed using the integrated grid velocities and then used to 
compute the deformation gradient. The new quantities introduced in this section are
\begin{itemize} 
  \setlength\itemsep{1pt}
  \item $\Delta\BF_{p}^n$ : {\Ochre The increment of the particle deformation gradient.}
  \item $\BlT_p^{n+1}$ : {\Ochre The particle velocity gradient.}
  \item $\rho_0$ : {\Ochre The initial mass density of the material.}
\end{itemize}
\begin{breakablealgorithm}
  \caption{Computing the velocity gradient and deformation gradient}
  \begin{algorithmic}[1]
    \Require $\Delta t$, $\Bx_p^n$, $m_p$, $V_p^n$, $\Bh_p^n$, $\Bv_p^n$, $\BlT_p^n$, $\BF_p^n$,
             $\Bh_g$, $\Bv_g$, $\Bv_g^\star$, $\rho_0$
             \TTList{materialList}, \TTList{gridNodeList},
             \TTObj{mpmFlags}, \TTObj{velGradComputer}
    \Procedure{computeDeformationGradient}{}
      \State \TTObj{interpolator} $\leftarrow$ \textsc{createInterpolator}(\TTObj{mpmFlags})
      \For{\TTmatl\, \textbf{in} \TTList{materialList}}
        \For{\TTpart\, \textbf{in} \TTList{particleList}}
          \State $\BlT_p^{n+1}$[\TTmatl,\TTpart] $\leftarrow$ 
           \TTObj{velGradComputer}.\textsc{computeVelGrad}(\TTObj{interpolator},
            $\Bh_g$, $\Bx_p^n$[\TTmatl,\TTpart], \WWRP
            $\Bh_p^n$[\TTmatl,\TTpart], $\BF_p^n$[\TTmatl,\TTpart], 
            $\Bv_g^\star$[\TTmatl])
            \Comment{Compute the velocity gradient}
          \State $\BF_p^{n+1}$[\TTmatl,\TTpart], $\Delta \BF_p^{n+1}$ $\leftarrow$ 
            \textsc{computeDeformationGradientFromVelocity}($\BlT_p^n$[\TTmatl,\TTpart], \WWRP
            $\BlT_p^{n+1}$[\TTmatl,\TTpart], $\BF_p^n$[\TTmatl,\TTpart])
            \Comment{Compute the deformation gradient}
          \State $V_p^{n+1}$[\TTmatl,\TTpart] $\leftarrow$ $m_p$[\TTmatl,\TTpart]/$\rho_0$ 
            $*\det(\BF_p^{n+1}$[\TTmatl,\TTpart])
        \EndFor
      \EndFor
      \State \Return $\BlT_p^{n+1}$, $\BF_p^{n+1}$, $V_p^{n+1}$
    \EndProcedure
  \end{algorithmic}
\end{breakablealgorithm}

\begin{breakablealgorithm}
  \caption{Computing the deformation gradient using the velocity gradient}
  \begin{algorithmic}[1]
    \Require $\Delta t$, $\BlT_p^{n+1}$, $\BF_p^n$,
             \TTObj{mpmFlags}
    \Procedure{computeDeformationGradientFromVelocity}{}
      \If {\TTObj{mpmFlags}.\texttt{defGradAlgorithm} = \texttt{"first\_order"}}
         \State $\BF_p^{n+1}$, $\Delta\BF_p^{n+1}$  $\leftarrow$ 
           \textsc{seriesUpdateConstantVelGrad}(\texttt{numTerms} = 1, $\Delta t$, $\BlT_p^{n+1}$, $\BF_p^n$) 
      \ElsIf {\TTObj{mpmFlags}.\texttt{defGradAlgorithm} = \texttt{"subcycle"}}
         \State $\BF_p^{n+1}$, $\Delta\BF_p^{n+1}$  $\leftarrow$
           \textsc{subcycleUpdateConstantVelGrad}($\Delta t$, $\BlT_p^{n+1}$, $\BF_p^n$) 
      \ElsIf {\TTObj{mpmFlags}.\texttt{defGradAlgorithm} = \texttt{"taylor\_series"}}
         \State $\BF_p^{n+1}$, $\Delta\BF_p^{n+1}$  $\leftarrow$
           \textsc{seriesUpdateConstantVelGrad}(\texttt{numTerms} = \TTObj{mpmFlags}.\texttt{numTaylorSeriesTerms}, 
              $\Delta t$, $\BlT_p^{n+1}$, $\BF_p^n$) 
      \Else
         \State $\BF_p^{n+1}$, $\Delta\BF_p^{n+1}$  $\leftarrow$
           \textsc{cayleyUpdateConstantVelGrad}($\Delta t$, $\BlT_p^{n+1}$, $\BF_p^n$) 
      \EndIf
      \State \Return $\BF_p^{n+1}$, $\Delta\BF_p^{n+1}$ 
    \EndProcedure
  \end{algorithmic}
\end{breakablealgorithm}

\subsection{Computing the stress tensor}
The stress tensor is compute by individual constitutive models.  Details of the Arena partially
saturated model are given later. The new quantities introduced in this section are
\begin{itemize} 
  \setlength\itemsep{1pt}
  \item $\Beta_p^n, \Beta_p^{n+1}$ : {\Ochre The internal variables needed by the constitutive model.}
\end{itemize}
\begin{breakablealgorithm}
  \caption{Computing the stress tensor}
  \begin{algorithmic}[1]
    \Require $\Delta t$, $\Bx_p^n$, $m_p$, $V_p^{n+1}$, $\Bh_p^n$, $\BlT_p^{n+1}$, $\BF_p^{n+1}$,
             $\Bsig_p^n$, $\Beta_p^n$, $\rho_0$, \TTList{materialList}, \TTObj{mpmFlags}, \TTObj{constitutiveModel}
    \Procedure{computeStressTensor}{}
      \For{\TTmatl\, \textbf{in} \TTList{materialList}}
        \State $\Bsig^{n+1}$, $\Beta_p^{n+1}$ $\leftarrow$
          \TTObj{constitutiveModel}[\TTmatl].\textsc{computeStressTensor}($\Delta t$, $\Bx_p^n$, $m_p$, 
             $V_p^{n+1}$, $\Bh_p^n$, \WWRP 
             $\BlT_p^{n+1}$, $\BF_p^{n+1}$, $\Bsig_p^n$, $\Beta_p^n$, $\rho_0$, \TTObj{mpmFlags})
          \Comment{Update the stress and any \WWRP internal variables needed by the constitutive model}
      \EndFor
      \State \Return $\Bsig_p^{n+1}$, $\Beta_p^{n+1}$
    \EndProcedure
  \end{algorithmic}
\end{breakablealgorithm}

\subsection{Computing the basic damage parameter}
The damage parameter is updated and the particle stress is modified in this procedure. The new 
quantities introduced in this section are
\begin{itemize} 
  \setlength\itemsep{1pt}
  \item $\Veps_p^{f,n}, \Veps_p^{f,n+1}$ : {\Ochre The particle strain to failure at $t = T_n$ 
        and $t = T_{n+1}$.}
  \item $\chi_p^{n}, \chi_p^{n+1}$ : {\Ochre An indicator function that identifies whether a particle
        has failed completely.} 
  \item $t_p^{\chi,n}, t_p^{\chi,n+1}$ : {\Ochre The time to failure of a particle.}
  \item $D_p^{n}, D_p^{n+1}$ : {\Ochre A particle damage parameter that can be used to modify the stress.}
\end{itemize}
\begin{breakablealgorithm}
  \caption{Computing the damage parameter}
  \begin{algorithmic}[1]
    \Require $t^{n+1}$, $V_p^{n+1}$, $\BF_p^{n+1}$, $\Bsig_p^{n+1}$, $D_p^n$, $\Veps_p^{f,n}$, $\chi^n_p$, 
             $t^{\chi,n}_p$,
             \TTList{materialList}, \TTObj{mpmFlags}
    \Procedure{computeDamage}{}
      \For{\TTmatl\, \textbf{in} \TTList{materialList}}
        \For{\TTpart\, \textbf{in} \TTList{particleList}}
          \If {\texttt{brittleDamage} = TRUE} 
            \State $\Bsig_p^{n+1}$, $\Veps_p^{f,n+1}$, $\chi^{n+1}_p$, $t^{\chi,n+1}_p$, $D_p^{n+1}$ $\leftarrow$
               \textsc{updateDamageAndModifyStress}($V_p^{n+1}$, $\BF_p^{n+1}$, \WWRP 
                  $\Bsig_p^{n+1}$, $D_p^n$, $\Veps_p^{f,n}$, $\chi^n_p$, $t^{\chi,n}_p$)
             \Comment{Update the damage parameters and stress}
          \Else
            \State $\Bsig_p^{n+1}$, $\Veps_p^{f,n+1}$, $\chi^{n+1}_p$, $t^{\chi,n+1}_p$ $\leftarrow$
               \textsc{updateFailedParticlesAndModifyStress}($V_p^{n+1}$, $\BF_p^{n+1}$, \WWRP 
                  $\Bsig_p^{n+1}$, $\Veps_p^{f,n}$, $\chi^n_p$, $t^{\chi,n}_p$, $t^{n+1}$)
             \Comment{Update the failed particles and stress}
          \EndIf
        \EndFor
      \EndFor
      \State \Return $\Bsig_p^{n+1}$, $\Veps_p^{f,n+1}$, $\chi^{n+1}_p$, $t^{\chi,n+1}_p$, $D_p^{n+1}$
    \EndProcedure
  \end{algorithmic}
\end{breakablealgorithm}

\subsection{Updating the particle erosion parameter}
The particle failure indicator function is updated in this procedure and used later for 
particle deletion if needed.
\begin{breakablealgorithm}
  \caption{Updating the particle erosion parameter}
  \begin{algorithmic}[1]
    \Require $D_p^n$, $\chi^n_p$
             \TTList{materialList}, \TTObj{mpmFlags}, \TTObj{constitutiveModel}
    \Procedure{updateErosionParameter}{}
      \For{\TTmatl\, \textbf{in} \TTList{materialList}}
        \For{\TTpart\, \textbf{in} \TTList{particleList}}
          \If {\TTmatl.\texttt{doBasicDamage} = TRUE} 
            \State $\chi^{n+1}_p$ $\leftarrow$
               \TTObj{damageModel}.\textsc{getLocalizationParameter}()
             \Comment{Just get the indicator \WWRP parameter for particles that will be eroded.}
          \Else
            \State $\chi^{n+1}_p$, $D_p^{n+1}$ $\leftarrow$
               \TTObj{constitutiveModel}[\TTmatl].\textsc{getDamageParameter}($\chi^n_p$, $D^n_p$)
             \WWRP \Comment{Update the damage parameter in the constitutive model.}
          \EndIf
        \EndFor
      \EndFor
      \State \Return $\chi^{n+1}_p$, $D_p^{n+1}$
    \EndProcedure
  \end{algorithmic}
\end{breakablealgorithm}

\subsection{Interpolating back to the particles and update}
This is the final step at which the particle velocities and positions are updated and the grid
is reset.  Particle that are to be removed are dealt with in a subsequent \texttt{relocation} step.
\begin{breakablealgorithm}
  \caption{Interpolating back to the particles and position update}
  \begin{algorithmic}[1]
    \Require $\Delta t$, 
             $\Ba_g$, $\Bv_g^\star$, $\Bx_p^n$, $\Bv_p^n$, $\Bu_p^n$, $\Bh_p^n$, $\chi_p^{n+1}$, $\BF_p^{n+1}$,
             $V_p^{n+1}$,
             \TTList{materialList}, \TTList{particleList}, \TTList{gridNodeList}, \TTObj{mpmFlags}
    \Procedure{interpolateToParticlesAndUpdate}{}
      \State \texttt{interpolator} $\leftarrow$ \textsc{createInterpolator}(\TTObj{mpmFlags})
      \For{\TTmatl\, \textbf{in} \TTList{materialList}}
        \State $\Bh_p^{n+1}$ $\leftarrow$ $\Bh_p^n$
        \For{\TTpart\, \textbf{in} \TTList{particleList}}
          \State $n_{gp}$, $S_{gp}$ $\leftarrow$ 
            \texttt{interpolator}.\textsc{findCellsAndWeights}($\Bx_p^n$, $\Bh_p^{n+1}$, $\BF_p^{n+1}$)
          \State $\Bv$ $\leftarrow$ $\Bzero$,~~ $\Ba$ $\leftarrow$ $\Bzero$,~
          \For{\TTnode\, \textbf{in} \TTList{gridNodeList}}
            \State $\Bv$ $\leftarrow$ $\Bv$ + $\Bv_g^\star$[\TTnode] $*$ $S_{gp}$[\TTnode]
              \Comment{Update particle velocity}
            \State $\Ba$ $\leftarrow$ $\Ba$ + $\Ba_g$[\TTnode] $*$ $S_{gp}$[\TTnode]
              \Comment{Update particle acceleration}
          \EndFor
          \State $\Bx_p^{n+1}$ $\leftarrow$ $\Bx_p^n$ + $\Bv * \Delta t$
            \Comment{Update position}
          \State $\Bu_p^{n+1}$ $\leftarrow$ $\Bu_p^n$ + $\Bv * \Delta t$
            \Comment{Update displacement}
          \State $\Bv_p^{n+1}$ $\leftarrow$ $\Bv_p^n$ + $\Ba * \Delta t$
            \Comment{Update velocity}
        \EndFor
      \EndFor
      \State \textsc{deleteRogueParticles}()
        \Comment{Delete particles that are to be eroded.}
      \State \Return $V_p^{n+1}$, $\Bu_p^{n+1}$, $\Bv_p^{n+1}$, $\Bx_p^{n+1}$, $m_p$, $\Bh_p^{n+1}$
    \EndProcedure
  \end{algorithmic}
\end{breakablealgorithm}

\section{Vaango Implementation} \label{Sec:UintahImp}
This section is intended to serve as a reference for users who find themselves needing
to modify the source code, or those who are simply interested.  

The goal of this section is to provide a mapping from the the algorithm described above
to the software that carries it out.  This won't be exhaustive, but will be a
good starting point for the motivated reader.

The source code for the \Vaango-MPM implementation can be found in

\begin{lstlisting}[backgroundcolor=\color{background}]
src/CCA/Components/MPM
\end{lstlisting}
Within that directory are a number of files and subdirectories, these will be
discussed as needed.  For the moment, consider the various files that end in
``{MPM.cc}":
\begin{lstlisting}[backgroundcolor=\color{background}]
AMRMPM.cc  FractureMPM.cc  ImpMPM.cc  RigidMPM.cc  SerialMPM.cc  ShellMPM.cc
\end{lstlisting}

\Textsfc{AMRMPM.cc} is the nascent beginnings of an AMR implementation of MPM. 
It is far from complete and should be ignored.  \Textsfc{FractureMPM.cc} is an implementation of
the work of Guo and Nairn \cite{GuoNairn}, and while it is viable, it is
undocumented and unsupported.  \Textsfc{ShellMPM.cc} is a treatment of MPM particles
as shell and membrane elements, developed by Biswajit Bannerjee.  It is also
viable, but also undocumented and unsupported.  \Textsfc{ImpMPM.cc} is an implicit
time integration form of MPM based on the work of Guilkey and Weiss
\cite{Guilkey2003}.  It is also viable, and future releases of \Vaango will include
documentation of its capabilities and uses.  For now, interested readers
should contact Jim Guilkey directly for more information.  \Textsfc{RigidMPM.cc} contains
a very reduced level of functionality, and is used solely in conjunction with
the MPMArches component.

This leaves \Textsfc{SerialMPM.cc}  This contains, despite its name, the parallel
implementation of the algorithm described earlier and in the \Vaango Theory Manual.
For now, we will skip over the initialization procedures such as:
\begin{lstlisting}[language=Cpp]
SerialMPM::problemSetup
SerialMPM::scheduleInitialize
SerialMPM::actuallyInitialize
\end{lstlisting}
and focus mainly on the timestepping algorithm described above.  

Each of the \Vaango components contains a function called 
\Textsfc{scheduleTimeAdvance} The algorithms implemented in these
components are broken into a number of steps.  The implementation of
these steps in \Vaango take place in ``tasks".  Each task is
responsible for performing the calculations needed to accomplish that
step in the algorithm.  Thus, each task requires some data upon which
to operate, and it also creates some data, either as a final result,
or as input to a subsequent task.  Before individual tasks are
executed, each is first ``scheduled".  The scheduling of tasks
describes the dataflow and data dependencies for a given algorithm.
By describing the data dependencies, both temporally and spatially,
each task can be executed in the proper order, and communication tasks
can automatically be generated by the \Vaango infrastructure to achieve
parallelism.  Thus, scheduleTimeAdvance calls a series of functions,
each of which schedules the individual tasks.  Let's begin by looking
at the \Textsfc{scheduleTimeAdvance} for \Textsfc{SerialMPM}
pasted below.

\begin{lstlisting}[language=Cpp]
void
SerialMPM::scheduleTimeAdvance(const LevelP & level,
                               SchedulerP   & sched)
{
  MALLOC_TRACE_TAG_SCOPE("SerialMPM::scheduleTimeAdvance()");
  if (!flags->doMPMOnLevel(level->getIndex(), level->getGrid()->numLevels()))
    return;

  const PatchSet* patches = level->eachPatch();
  const MaterialSet* matls = d_sharedState->allMPMMaterials();

  scheduleApplyExternalLoads(             sched, patches, matls);
  scheduleInterpolateParticlesToGrid(     sched, patches, matls);
  scheduleExMomInterpolated(              sched, patches, matls);
  scheduleComputeContactArea(             sched, patches, matls);
  scheduleComputeInternalForce(           sched, patches, matls);

  scheduleComputeAndIntegrateAcceleration(sched, patches, matls);
  scheduleExMomIntegrated(                sched, patches, matls);
  scheduleSetGridBoundaryConditions(      sched, patches, matls);
  scheduleSetPrescribedMotion(            sched, patches, matls);
  scheduleComputeStressTensor(            sched, patches, matls);
  if(flags->d_doExplicitHeatConduction){
    scheduleComputeHeatExchange(          sched, patches, matls);
    scheduleComputeInternalHeatRate(      sched, patches, matls);
    scheduleComputeNodalHeatFlux(         sched, patches, matls);
    scheduleSolveHeatEquations(           sched, patches, matls);
    scheduleIntegrateTemperatureRate(     sched, patches, matls);
  }
  scheduleAddNewParticles(                sched, patches, matls);
  scheduleConvertLocalizedParticles(      sched, patches, matls);
  scheduleInterpolateToParticlesAndUpdate(sched, patches, matls);

  if(flags->d_canAddMPMMaterial){
    //  This checks to see if the model on THIS patch says that it's
    //  time to add a new material
    scheduleCheckNeedAddMPMMaterial(         sched, patches, matls);

    //  This one checks to see if the model on ANY patch says that it's
    //  time to add a new material
    scheduleSetNeedAddMaterialFlag(         sched, level,   matls);
  }

  sched->scheduleParticleRelocation(level, lb->pXLabel_preReloc,
                                    d_sharedState->d_particleState_preReloc,
                                    lb->pXLabel,
                                    d_sharedState->d_particleState,
                                    lb->pParticleIDLabel, matls);
  if(d_analysisModule){
    d_analysisModule->scheduleDoAnalysis( sched, level);
  }
}
\end{lstlisting}

The preceding includes scheduling for a number of rarely used features.
For now, let's condense the preceding to the essential tasks:

\begin{lstlisting}[language=Cpp]
void
SerialMPM::scheduleTimeAdvance(const LevelP & level,
                               SchedulerP   & sched)
{
  if (!flags->doMPMOnLevel(level->getIndex(), level->getGrid()->numLevels()))
    return;

  const PatchSet* patches = level->eachPatch();
  const MaterialSet* matls = d_sharedState->allMPMMaterials();

  scheduleApplyExternalLoads(             sched, patches, matls);
  scheduleInterpolateParticlesToGrid(     sched, patches, matls);
  scheduleExMomInterpolated(              sched, patches, matls);
  scheduleComputeInternalForce(           sched, patches, matls);

  scheduleComputeAndIntegrateAcceleration(sched, patches, matls);
  scheduleExMomIntegrated(                sched, patches, matls);
  scheduleSetGridBoundaryConditions(      sched, patches, matls);
  scheduleComputeStressTensor(            sched, patches, matls);
  scheduleInterpolateToParticlesAndUpdate(sched, patches, matls);

  sched->scheduleParticleRelocation(level, lb->pXLabel_preReloc,
                                    d_sharedState->d_particleState_preReloc,
                                    lb->pXLabel,
                                    d_sharedState->d_particleState,
                                    lb->pParticleIDLabel, matls);
}
\end{lstlisting}

As described above, each of the ``schedule" functions describes
dataflow, and it also calls the function that actually executes the
task.  The naming convention is illustrated by an example, 
\Textsfc{scheduleComputeAndIntegrateAcceleration} calls 
\Textsfc{computeAndIntegrateAcceleration} Let's examine this
particular task more carefully.  First, the scheduling of the
task:

\begin{lstlisting}[language=Cpp]
void SerialMPM::scheduleComputeAndIntegrateAcceleration(SchedulerP& sched,
                                                       const PatchSet* patches,
                                                       const MaterialSet* matls)
{
  if (!flags->doMPMOnLevel(getLevel(patches)->getIndex(),
                           getLevel(patches)->getGrid()->numLevels()))
    return;

  printSchedule(patches,cout_doing,"MPM::scheduleComputeAndIntegrateAcceleration\t\t\t\t");

  Task* t = scinew Task("MPM::computeAndIntegrateAcceleration",
                        this, &SerialMPM::computeAndIntegrateAcceleration);

  t->requires(Task::OldDW, d_sharedState->get_delt_label() );

  t->requires(Task::NewDW, lb->gMassLabel,          Ghost::None);
  t->requires(Task::NewDW, lb->gInternalForceLabel, Ghost::None);
  t->requires(Task::NewDW, lb->gExternalForceLabel, Ghost::None);
  t->requires(Task::NewDW, lb->gVelocityLabel,      Ghost::None);

  t->computes(lb->gVelocityStarLabel);
  t->computes(lb->gAccelerationLabel);

  sched->addTask(t, patches, matls);
}
\end{lstlisting}

The \Textsfc{if} statement basically directs the schedule to only do this task on the 
finest level (MPM can be used in AMR simulations, but only at the finest
level.)  The \Textsfc{printSchedule} command is in place for debugging purposes,
this type of print statement can be turned on by setting an environmental
variable.  The real business of this task begins with the declaration of the
Task.  In the task declaration, the function associated with that task is
identified.  Subsequent to that is a description of the data dependencies.
Namely, this task \Textsfc{requires} the mass, internal and external forces as well
as velocity on the grid.  No ghost data are required as this task is a 
node by node calculation.  It also requires the timestep size.  Note also
that most of the required data are needed from the \Textsfc{NewDW} where
\Textsfc{DW} refers to
DataWarehouse.  This simply means that these data were calculated by an
earlier task in the current timestep.  The timestep size for this step
was computed in the previous timestep, and thus is required from the \Textsfc{OldDW}
Finally, this task \Textsfc{computes} the acceleration and time advanced velocity
at each node.

The code to execute this task is as follows:

\begin{lstlisting}[language=Cpp]
void SerialMPM::computeAndIntegrateAcceleration(const ProcessorGroup*,
                                                const PatchSubset* patches,
                                                const MaterialSubset*,
                                                DataWarehouse* old_dw,
                                                DataWarehouse* new_dw)
{
  for(int p=0;p<patches->size();p++){
    const Patch* patch = patches->get(p);
    printTask(patches, patch,cout_doing,"Doing computeAndIntegrateAcceleration\t\t\t\t");

    Ghost::GhostType  gnone = Ghost::None;
    Vector gravity = d_sharedState->getGravity();
    for(int m = 0; m < d_sharedState->getNumMPMMatls(); m++){
      MPMMaterial* mpm_matl = d_sharedState->getMPMMaterial( m );
      int dwi = mpm_matl->getDWIndex();

      // Get required variables for this patch
      constNCVariable<Vector> internalforce, externalforce, velocity;
      constNCVariable<double> mass;

      delt_vartype delT;
      old_dw->get(delT, d_sharedState->get_delt_label(), getLevel(patches) );

      new_dw->get(internalforce,lb->gInternalForceLabel, dwi, patch, gnone, 0);
      new_dw->get(externalforce,lb->gExternalForceLabel, dwi, patch, gnone, 0);
      new_dw->get(mass,         lb->gMassLabel,          dwi, patch, gnone, 0);
      new_dw->get(velocity,     lb->gVelocityLabel,      dwi, patch, gnone, 0);

      // Create variables for the results
      NCVariable<Vector> velocity_star,acceleration;
      new_dw->allocateAndPut(velocity_star, lb->gVelocityStarLabel, dwi, patch);
      new_dw->allocateAndPut(acceleration,  lb->gAccelerationLabel, dwi, patch);

      acceleration.initialize(Vector(0.,0.,0.));
      double damp_coef = flags->d_artificialDampCoeff;

      for(NodeIterator iter=patch->getExtraNodeIterator__New();
                        !iter.done();iter++){
        IntVector c = *iter;
        Vector acc(0.,0.,0.);
        if (mass[c] > flags->d_min_mass_for_acceleration){
          acc  = (internalforce[c] + externalforce[c])/mass[c];
          acc -= damp_coef*velocity[c];
        }
        acceleration[c] = acc +  gravity;
        velocity_star[c] = velocity[c] + acceleration[c] * delT;
      }
    }    // matls
  }
}
\end{lstlisting}

This task contains three nested for loops.  First, is a loop over all of the
``patches" that the processor executing this task is responsible for.  Next
is a loop over all materials (imagine a simulation involving the interaction
between, say, tungsten and copper).  Within this loop, the required data
are retrieved from the \Textsfc{new\_dw} (New DataWarehouse) and space for the data
to be created is allocated.  The final loop is over all of the nodes on
the current patch, and the calculations described by
Equations~\ref{MPM:acceleration_upd} and~\ref{MPM:euler_upd} are carried out.  (This
also includes a linear damping term not described above.)
\Beq
  \bfa_i = \frac{\bfF_i^{\rm{ext}} - \bfF_i^{\rm{int}}}{m_i}
  \label{MPM:acceleration_upd}
\Eeq
An explicit forward Euler method is used for the time integration:
\Beq
  \bfv_i^L= \bfv_i + \bfa_i \Delta{t}
  \label{MPM:euler_upd}
\Eeq

Let's consider each task in turn.  The remaining tasks will be described
in much less detail, but the preceding dissection of a fairly simple task,
along with a description of what the remaining tasks are intended to 
accomplish, should allow interested individuals to follow the remainder
of the \Vaango-MPM implementation.

\begin{enumerate}
\item {\Textsfc{scheduleApplyExternalLoads}} This task is mainly responsible for
applying traction boundary conditions described in the input file.  This is
done by assigning external force vectors to the particles.  If the user
wishes to apply a load that is not possible to acheive via the input file
options, it is straightforward to modify the code here to do ``one-off" tests.

\item {\Textsfc{scheduleInterpolateParticlesToGrid}}   The name of this task was
poorly chosen, but has persisted.  This task carries out the operations given
in Equation~\ref{accumulate_upd}.  It also sets boundary conditions on some of
the variables, such as the grid temperature, and grid velocity 
(in the case of symmetry BCs).
\Beq
m_i = \sum_{p} S_{ip} m_p,  \quad
\bfv_i = \frac{\sum\limits_{p} S_{ip} m_p \bfv_p}{m_i},  \quad
\bfF^{\rm{ext}}_i = \sum_{p} S_{ip} \bfF^{\rm{ext}}_p
\label{accumulate_upd}
\Eeq
where $i$ refers to individual nodes of the grid,  $m_p$ is the particle
mass, $\bfv_p$ is the particle velocity, and $\bfF^{\rm{ext}}_p$ is the
external force on the particle.

\item {\Textsfc{scheduleExMomInterpolated}}  This task actually exists in one
of the contact models which can be found in the \Textsfc{Contact} directory.  Each of those
models has two main tasks. This is the the first of those. It is responsible
for modifying the grid velocity computed by interpolateParticlesToGrid according
to the rules for the particular contact model chosen in the input file.  These
models are briefly described in the \Vaango Theory Manual.

\item {\Textsfc{scheduleComputeInternalForce}} This task computes the volume
integral of the divergence of stress.  Specifically, it carries out the
operation given in Equation~\ref{computeIntForce_upd}.  It also computes some
diagnostic data, if requested in the input file, such as the reaction forces
(tractions) on the boundaries of the computational domain.
\begin{eqnarray}
\bfF^{\rm{int}}_i &=& \sum_{p} \bfG_{ip} \sig_p v_p,
\label{computeIntForce_upd}  
\end{eqnarray}
where $\bfG_{ip}$ is the gradient of the shape function of the $ith$ node
evaluated at $\bfx_p$, and $\sig_p$ and $v_p$ are the time $n$ values of
particle stress and volume respectively.  

\item {\Textsfc{scheduleComputeAndIntegrateAcceleration}} As described previously,
this task carries out the operations described in 
Equations~\ref{MPM:acceleration_upd} and~\ref{MPM:euler_upd}.

\item {\Textsfc{scheduleExMomIntegrated}}  This is the second of the contact tasks
(see above).  It is responsible for modifiying the time advanced grid velocity
computed in \Textsfc{computeAndIntegrateAcceleration}

\item {\Textsfc{scheduleSetGridBoundaryConditions}}  This task sets boundary conditions
on the time advanced grid velocity.  It also sets an acceleration boundary
condition as well.  However, rather than just setting the acceleration
to a given value, it is computed by solving Equation~\ref{MPM:euler_upd} for
acceleration, and then recomputing the acceleration (on all nodes) as:
\Beq
  \bfa_i= \frac{\bfv^L_i - \bfv_i}{\Delta{t}}
  \label{MPM:accBC}
\Eeq
Doing this operation on all nodes has several advantages.  For most interior
nodes, the value for acceleration will be unchanged, but for nodes on the 
where the velocity has been altered by enforcing boundary conditions, and
for nodes at which the contact models have altered the velocity, the acceleration
will be modified to reflect that alteration.

\item{\Textsfc{scheduleComputeStressTensor}}  The task, \Textsfc{computeStressTensor}
exists in each of the models in the \Textsfc{ConstitutiveModel} directory.  Each
model is responsible for carrying out the operations given in
Equation~\ref{p_vol_upd}, 
\Beq
  v_p^{n+1} = \rm{Det}(\bfd\bfF_p^{n+1})v_p^n,  \quad
  \bfF_p^{n+1}=\bfd\bfF_p^{n+1} \bfF_{p}^{n}
  \label{p_vol_upd}
\Eeq
and of course, as the name implies, it also computes
the material stress.  This task has one additional important function,
and that is computing the timestep size for the subsequent step.  The CFL
condition dictates that the timestep size be limited according to:
\Beq
  \Delta{t} < \frac{\Delta{x}}{c+|u|}
  \label{MPM:CFL}
\Eeq
where $\Delta{x}$ is the cell spacing, $c$ is the wavespeed in the material,
and $|u|$ is the magnitude of the local velocity.  Because the wavespeed 
may depend on the state of stress that a material is in, this task provides
a convenient time at which to make this calculation.  A timestep size is
computed for all particles, and the minimum for the particle set on a given
patch is put into a ``reduction variable".  The \Vaango infrastructure then
does a global reduction to select the smallest timestep from across the
domain.

\item {\Textsfc{scheduleInterpolateToParticlesAndUpdate}}  This task carries out the
operations in Equations~\ref{MPM:updateVp_upd} and~\ref{MPM:updateXp_upd}, namely updating
the particle state based on the grid solution.
\Beq
  \bfv_p (t + \Delta{t})  = \bfv_p (t)  + \sum_{i} S_{ip} \bfa_i  \Delta{t} 
  \label{MPM:updateVp_upd}
\Eeq
\Beq
  \bfx_p (t + \Delta{t})  = \bfx_p (t)  + \sum_{i} S_{ip} \bfv^L_i  \Delta{t}
  \label{MPM:updateXp_upd}
\Eeq

\item {\Textsfc{scheduleParticleRelocation}}  This task is not actually located
in the MPM code, but in the \Vaango infrastructure.  The idea is that as particles
move, some will cross patch boundaries, and their data will need to be sent to
other processors.  This task is responsible for identifying particles that have
left the patch that they were on, finding where they went, and sending their
data to the correct processor.

\end{enumerate}
