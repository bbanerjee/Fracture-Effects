\section{Elastic materials}
Please refer to the \Vaango Theory Manual for brief descriptions of the theory used in 
these material models.

\subsection{Hypoelastic material}
A hypoelastic material model can be specified in the \Textsfc{UPS} input file using:
\begin{lstlisting}[language=XML]
  <constitutive_model type="hypo_elastic">
    <K> 32.0e6 </K>
    <G> 12.0e6 </G>
    <alpha> 1.0e-4 </alpha>
  </constitutive_model>
\end{lstlisting}

Here $K$ is the bulk modulus, $G$ is the shear modulus, and $\alpha$ is the coefficient
of thermal expansion.  If the Young's modulus ($E$) and Poisson's ratio ($\nu$) of the 
material are known, the bulk and shear modulus can be computed using
\Beq
  K = \frac{E}{3(1-2\nu)} \quad \Tand \quad
  G = \frac{E}{2(1+\nu)} \,.
\Eeq

\subsection{Compressible Mooney-Rivlin Model} This model is generally parameterized
for rubber type materials.  Usage is as follows:
\begin{lstlisting}[language=XML]
  <constitutive_model type="comp_mooney_rivlin">
    <he_constant_1>100000.0</he_constant_1>
    <he_constant_2>20000.0</he_constant_2>
    <he_PR>.49</he_PR>
  </constitutive_model>
\end{lstlisting}
where \Textsfc{\textless he\_constant\_(1,2)\textgreater} are usually referred to
as $C1$ and $C2$ in the literature, and \Textsfc{he\_PR} is the Poisson's ratio ($\nu$).
The initial shear modulus, $G$, is related to the two Mooney-Rivlin constants and the 
initial bulk modulus can be computed from $G$ and $\nu$ using
\Beq
  G = 2 (C_1 + C_2) \quad \Tand \quad K = \frac{2G(1+\nu)}{3(1-2\nu)} \,.
\Eeq

\subsection{Compressible Neo-Hookean Model} There are implementations of several
hyperelastic-plastic model described by Simo and Hughes\cite{Simo1998} (pp. 307 -- 321). 
 The model is dubbed "Unified Compressible Neo-Hookean Model" or UCNH for short.  Models can 
still be specified with old input file specifications, (i.e. comp\_neo\_hook, comp\_neo\_hook\_plastic,
cnh\_damage, cnhp\_damage) however these are merely wrappers for the underlying UCNH model.
 Plastic flow and failure can be modelled in addition to elasticity by  specifying 
several additional options with input flags. This models is very robust, and relatively 
straightforward because hyperelastic models don't require rotation back and forth 
between laboratory and material frames of reference.

\begin{NoteBox}
NOTE: Support for Implicit CNH and CNH with specified solver does not exist yet.
\end{NoteBox}

\subsubsection{Purely elastic}
For purely-elastic compressible neo-Hookean material simulations, the input has the form:
\begin{lstlisting}[language=XML]
  <constitutive_model type="UCNH"> 
    <bulk_modulus>   8.9e9  </bulk_modulus>
    <shear_modulus>  3.52e9 </shear_modulus>
    <useModifiedEOS> true   </useModifiedEOS>
  </constitutive_model>
\end{lstlisting}
Alternatively, this model can be invoked usig the \Textxml{comp\_neo\_hook} tag:
\begin{lstlisting}[language=XML]
  <constitutive_model type="comp_neo_hook"> 
    <bulk_modulus>   8.9e9  </bulk_modulus>
    <shear_modulus>  3.52e9 </shear_modulus>
    <useModifiedEOS> true   </useModifiedEOS>
  </constitutive_model>
\end{lstlisting}

\subsubsection{Elastic with brittle damage}
The \Textsfc{cnh\_damage} tag or the \Textxml{useDamage} tag
tells \Vaango to use a basic elastic model, with an extension
to failure based on a stress or strain as given below, thus yielding an
elastic-brittle failure model.  This model also allows a distribution
of failure strain (or stress) based on normal or Weibull distributions.
Note that the post-failure behaviour of simulations is not always robust.
The specification is:
\begin{lstlisting}[language=XML]
  <constitutive_model type="cnh_damage"> 
    <bulk_modulus>   8.9e9  </bulk_modulus>
    <shear_modulus>  3.52e9 </shear_modulus>
    <useModifiedEOS> true   </useModifiedEOS>
  </constitutive_model>
\end{lstlisting}
When specifying \Textsfc{cnh\_damage}, the material heterogeneity and damage 
specification described for the general model (UCNH) may also be specified
as discussed below.

\subsubsection{Elastic-Plastic ($J_2$-plasticity)}
For simulations with plasticity enabled, use
\begin{lstlisting}[language=XML]
  <constitutive_model type="UCNH"> 
    <!-- Necessary flags for all CNH models -->
    <bulk_modulus>   8.9e9  </bulk_modulus>
    <shear_modulus>  3.52e9 </shear_modulus>
    <useModifiedEOS> true   </useModifiedEOS>
                
    <!-- Plasticity Parameters -->
    <usePlasticity>     true  </usePlasticity>
    <yield_stress>      100.0 </yield_stress>
    <hardening_modulus> 500.0 </hardening_modulus>
    <alpha>             1.0   </alpha>
  </constitutive_model>
\end{lstlisting}
This model includes $J_2$-plasticity with isotropic linear hardening, 
and can be alternatively invoked using the \Textsfc{comp\_neo\_hook\_plastic} tag:
\begin{lstlisting}[language=XML]
  <constitutive_model type="comp_neo_hook_plastic">
    <bulk_modulus> 8.9e9 </bulk_modulus>
    <shear_modulus> 3.52e9 </shear_modulus>
    <useModifiedEOS> true </useModifiedEOS>
    <yield_stress> 100.0 </yield_stress>
    <hardening_modulus> 500.0 </hardening_modulus>
    <alpha> 1.0 </alpha>
  </constitutive_model>
\end{lstlisting}

\subsubsection{Elastic-Plastic with damage}
The \Textsfc{UCNH} model with damage can alternatively be invoked with the \Textsfc{cnhp\_damage}
tag. This constitutive model is an extension of the hyperelastic-plastic neo-Hookean model
to failure based on a stress or strain, thus yielding an elastic-plastic model with failure.  
Note that the post-failure behaviour of simulations is not always robust.  

When the \Textsfc{cnhp\_damage} tag is used instead of \Textsfc{UCNH}, the input section for 
damage and plasticity is similar to that for UCNH without \Textxml{useDamage} and 
\Textxml{usePlasticity}.

A fairly sophisticated means of seeding explicit material heterogeneity is also provided for. 
To use these features the following four steps are required: 
\begin{enumerate}
\item \Textmag{Erosion algorithm:} 
  To allow for failure (by material point erosion), in the \Textxml{MPM} block, the 
  \Textsfc{erosion algorithm} must be set to one of the following: 
  \begin{lstlisting}[language=XML]
    <erosion algorithm="AllowNoTension"/>
    <erosion algorithm="AllowNoShear"/>
    <erosion algorithm="ZeroStress"/>
  \end{lstlisting}
  In the \Textxml{constitutive\_model} block:
  \begin{lstlisting}[language=XML]
    <useDamage>true</useDamage>
  \end{lstlisting}

\item \Textmag{Failure criterion:} 
  The failure criterion must be specified.  This is also in the \Textxml{constitutive\_model}
  block.  One of the following must be specified:
  \begin{lstlisting}[language=XML]
    <failure_criteria>  MohrCoulomb </failure_criteria>
    <failure_criteria>  MaximumPrincipalStress </failure_criteria>
    <failure_criteria>  MaximumPrincipalStrain </failure_criteria>
  \end{lstlisting}

  The \Textsfc{MohrCoulomb failure criterion} is given by 
  \begin{equation}
  \frac{\sigma_3-\sigma_1}{2}=c\cos(\phi)-\frac{\sigma_3+\sigma_1}{2}\sin(\phi)
  \end{equation}
  where $\sigma_i$ are the ordered principal stresses, positive in tension 
  ($\sigma_3 > \sigma_2 > \sigma_1$).  Note, the MohrCoulomb failure 
  surface requires a friction angle, $\phi$, (in degrees):
  \begin{lstlisting}[language=XML]
    <friction_angle> friction angle </friction_angle>
  \end{lstlisting}
  and the \Textsfc{cohesion} ($c$) which is assigned using a distribution, as described below.  

  \begin{NoteBox}
  For the maximum 
  principal stress and strain failure criteria, the cohesion is the maximum value of principal 
  stress or strain that may be obtained (must be positive).  
  \end{NoteBox}

  A tensile cutoff failure surface may be added for MohrCoulomb.
  The tensile cutoff is taken to be a fraction of the cohesion.  
  This parameter is specified using:
  \begin{lstlisting}[language=XML]
    <tensile_cutoff_fraction> 0.1 </tensile_cutoff_fraction> 
  \end{lstlisting}
  Setting this to a large number effectively removes this failure surface, leaving just Mohr-Coulomb.

\item \Textmag{Material heterogeneity:}
  Material heterogeneity type must be specified.  For MohrCoulomb the cohesion is distributed 
  spatially (an independent assignment for each material point).  For MaximumPrincipalStress and 
  MaximumPrincipalStrain, the threshold stress or strain for failure, respectively, is distributed 
  spatially (an independent assignment for each material point).  Material heterogeneity is 
  distributed spatially by assigning values consistent with a distribution function.  Three different 
  distributions may be used.  All parameters are in the \Textxml{constitutive\_model} block:
  \begin{lstlisting}[language=XML]
    <failure_distrib> gauss </failure_distrib>
    <failure_distrib> weibull </failure_distrib>
    <failure_distrib> constant </failure_distrib>
  \end{lstlisting}

  A Gaussian \Textsfc{gauss} distribution requires the following parameters:
  \begin{lstlisting}[language=XML]
    <failure_mean> Gaussian mean value of cohesion </failure_mean>
    <failure_std> Gaussian standard deviation of cohesion </failure_std>
    <failure_seed> random number generator seed </failure_seed>
  \end{lstlisting}

  A Weibull (weibull) distribution requires the following parameters:
  \begin{lstlisting}[language=XML]
    <failure_mean> Weibull mean value of cohesion </failure_mean>
    <failure_std> Weibull modulus </failure_std>
    <failure_seed> random number generator seed </failure_seed>
  \end{lstlisting}

  A homogeneous (constant) assignment requires the following parameters:
  \begin{lstlisting}[language=XML]
    <failure_mean> value (all particles assigned one value) </failure_mean>
  \end{lstlisting}

\item \Textmag{Distribution scaling:}
  Distribution scaling with numerical resolution may optionally be specified.  This is only 
  available for Gaussian and Weibull distributions.  All parameters are in the 
  \Textxml{constitutive\_model} block:
  \begin{lstlisting}[language=XML]
    <scaling> kayenta </scaling>
    <scaling> none (default) </scaling>
  \end{lstlisting}

  For kayenta scaling, the mean value of the distribution is scaled by the factor
  \begin{equation}
   \biggl(\frac{\bar V}{V}\biggr)^{1/n}
  \end{equation}
  where $V$ is the particle volume, a function of numerical resolution.  The reference volume, 
  $\bar V$ and exponent, $n$, both must be specified
  \begin{lstlisting}[language=XML]
    <reference_volume> $\bar V$ </reference_volume>
    <exponent> n </exponent>
  \end{lstlisting}
  The exponent defaults to the Weibull modulus if the Weibull distribution is used.  This physically
  motivated scaling provides for an increase in mean cohesion with decreasing particle size, generally
  consistent with the observation that smaller quantities of material contain fewer critical flaws.
\end{enumerate}

\subsubsection{Post-failure behavior}
When a particle has failed, the value of the particle variable \Textsfc{p.localized}
will be larger than one (0 means the particle has not failed) and can be output in the 
\Textsfc{DataArchiver} section of the input file. In addition, the total number of failed particles as
a function of time \Textsfc{TotalLocalizedParticle} can be output. 

\subsubsection{Brittle damage}
Another damage model that can be used with \Textsfc{cnh\_damage} and
\Textsfc{cnhp\_damage} is a subset of the brittle damage model of LS-DYNA's Concrete
 Model 159 (FHWA-HRT-057-062, 2007). The model is invoked by the following MPMFlag
\begin{lstlisting}[language=XML]
  <erosion algorithm="BrittleDamage"/>
\end{lstlisting}
in the \Textxml{MPM} section of the input file. Two key features of the model are the use of
progressive (as opposed to sudden) damage due to softening to improve numerical stability, 
and the reduction of mesh size sensitivity via the specification of fracture energy. 

Brittle damage occurs when the mean stress $\sigma_{kk}/3$ is tensile
and the energy $\tau_b$, related to the maximum principal 
strain $\epsilon_{max}$, has exceeded a threshold value $r_0^b$
\begin{equation}
  \sigma_{kk}>0, \phantom{ijkl}
  \tau_b = \sqrt{E \epsilon_{max}^2} \geq r_0^b
\end{equation}
where $E$ is the Young's modulus. If at the next time step the mean stress is less than
zero (compressive), the damage mechanism can be optionally inactivated such that the current stress 
is set temporarily to a fraction of the undamaged stress to 
model stiffness recovery due to crack closing. When the mean stress becomes tensile again,
the value of the previous maximum damage $d$ can be restored; recovery is a user option in \Vaango
but should be used with caution since stiffening is more prone to instability.
The softening function for brittle damage is assumed to be
\begin{equation}
  d(\tau_b)= \frac{0.999}{D} \left(\frac{1+D}{1+D \exp^{-C(\tau_b-r_0^b)}} \right)
\end{equation}
where $C$ and $D$ are constants that define the shape of the softening
stress-strain curve.
  
To regulate mesh size sensitivity, the fracture energy
($G_f$), defined as the area under the stress-displacement
curve for displacement larger than $x_0$ (the displacement at peak strength), is to be 
maintained constant.
The user needs to input $G_f$ and $D$; $C$ is calculated internally. 

The maximum increment of damage that can accumulate over a single time
step is a user-defined input to avoid excessive damage accumulation over
a single time step to reduce numerical instability.

For \Textsfc{cnh\_damage}, the parameters for brittle damage can be specified as
\begin{lstlisting}[language=XML]
<constitutive_model type="cnh_damage">
  <shear_modulus>3.52e9</shear_modulus>
  <bulk_modulus>8.9e9</bulk_modulus>
  <brittle_damage_initial_threshold>57.0 </brittle_damage_initial_threshold>
  <brittle_damage_fracture_energy>11.2</brittle_damage_fracture_energy>
  <brittle_damage_constant_D>0.1</brittle_damage_constant_D>
  <brittle_damage_max_damage_increment>0.1</brittle_damage_max_damage_increment>
  <brittle_damage_allowRecovery> false </brittle_damage_allowRecovery>
  <brittle_damage_recoveryCoeff> 1.0 </brittle_damage_recoveryCoeff>
  <brittle_damage_printDamage> false </brittle_damage_printDamage>
</constitutive_model>
\end{lstlisting}

The tags in the input file for brittle damage are shown in the following table.
\begin{table}[ht]
\centering
\begin{tabular} {c c l}
\hline
Tag & Symbol & Description \\
\hline
brittle\_damage\_initial\_threshold & $r_0^b$ &  material property \\
brittle\_damage\_fracture\_energy & $G_f$ &  material property \\
brittle\_damage\_constant\_D & $D$ & material property \\
brittle\_damage\_max\_damage\_increment & & optional, default=0.1 \\
brittle\_damage\_allowRecovery & & allow crack closing (stiffening) \\
& & optional, default=false \\
brittle\_damage\_recoveryCoeff & & fraction of undamaged stress to recover\\
& & (between 0 and 1), optional\\
& & default=1.0 (full recovery) used only\\
& & when brittle\_damage\_allowRecovery \\
& & is set to true  \\
brittle\_damage\_printDamage & & print the state of damage \\
& & of damaged particles, default=false \\
& & (to reduce large amounts of output) \\
\hline
\end{tabular}
\end{table}

When a particle is damaged, the value of the particle variable \Textsfc{p.damage}
can be output in the \Textsfc{DataArchiver} section of the input file.

\section{Elastic modulus models}
For selected hypoelasticity-based material models, the model used to compute the bulk and shear modulus 
can be chosen separately.  Some of these elastic moduli models are discussed below.

\subsection{Support vector model}
The \Textsfc{tabular plasticity} model discussed later in this manual can be
combined with a support vector model for the bulk modulus.  
The support vector based elastic modulus model is specified using an input
description of the following form.
\begin{lstlisting}[language=XML]
  <constitutive_model type=".....">
    <elastic_moduli_model type="support_vector">
      <filename>SVR_fit.json</filename>
      <G0>3500</G0>
      <nu>0.189</nu>
    </elastic_moduli_model>
    ......
  </constitutive_model>
\end{lstlisting}
The JSON input file is required to contain a support vector regression fit to pressure data
as a function of the total volumetric strain and the plastic volumetric strain.  The format 
of the JSON file should be of the following form:
\begin{lstlisting}[language=JSON]
{
  "X_var": ["Total volumetric strain (\%)", "Plastic volumetric strain (\%)"],
  "y_var": "Pressure (MPa)",
  "X_conversion_factor": [0.01, 0.01],
  "y_conversion_factor": 1000000.0,
  "X_scale": [0.018115942028985508, 0.031249261081060988],
  "X_min": [-10.0, 0.0],
  "X_max": [45.2, 32.00075667088534],
  "y_scale": [0.00016356411948678396],
  "y_min": [-1112.690309606444],
  "y_max": [5001.12],
  "gamma": 4.7474110414223025,
  "support_vectors": [[0.9998641304347827, 1.0], [0.8832644927536233, 1.0],
    [0.9954365942028985, 1.0], [0.6534438405797102, 0.7060732743351001],
    [0.8108822463768116, 1.0], [0.7982101449275363, 0.943522446672424], ...],
  "dual_coeffs": [[10.0, 10.0, 10.0, -10.0, 10.0, 10.0, 2.5932059232147773, 10.0, ...],
  "intercept": [0.8428855972115061]
}
\end{lstlisting}
An initial value of \Textsfc{G0}
is used if \Textsfc{nu} is less than -1.0 or greater than 0.5.  Otherwise the shear modulus is 
computed from the bulk modulus model using the value of \Textsfc{nu} at the Poisson's ratio.

An example of fitting a support vector regression model in \Textsfc{Python} is shown below.
\begin{lstlisting}[language=Python]
#!/usr/bin/env python
# coding: utf-8
#
# Load packages
#
import numpy as np
from sklearn.svm import SVR, NuSVR
from sklearn import preprocessing
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
import pandas as pd
import matplotlib.pyplot as plt
import json
from json import JSONEncoder
import sys
#
# Load input data from CSV files
#
unload_09 = pd.read_csv("./Sand_unload_09.csv", header=0, skiprows=1)
unload_18 = pd.read_csv("./Sand_unload_18.csv", header=0, skiprows=1)
unload_27 = pd.read_csv("./Sand_unload_27.csv", header=0, skiprows=1)
unload_36 = pd.read_csv("./Sand_unload_36.csv", header=0, skiprows=1)
unload_45 = pd.read_csv("./Sand_unload_45.csv", header=0, skiprows=1)
#
# Compute plastic strains from intersection with strain axis
#
t_09 = -unload_09.iloc[-1,1]/(unload_09.iloc[-4,1] - unload_09.iloc[-1,1])
t_18 = -unload_18.iloc[-1,1]/(unload_18.iloc[-4,1] - unload_18.iloc[-1,1])
t_27 = -unload_27.iloc[-1,1]/(unload_27.iloc[-4,1] - unload_27.iloc[-1,1])
t_36 = -unload_36.iloc[-1,1]/(unload_36.iloc[-4,1] - unload_36.iloc[-1,1])
t_45 = -unload_45.iloc[-1,1]/(unload_45.iloc[-4,1] - unload_45.iloc[-1,1])
eps_p_09 = (1 - t_09) * unload_09.iloc[-1,0] + t_09 * unload_09.iloc[-4,0]
eps_p_18 = (1 - t_18) * unload_18.iloc[-1,0] + t_18 * unload_18.iloc[-4,0]
eps_p_27 = (1 - t_27) * unload_27.iloc[-1,0] + t_27 * unload_27.iloc[-4,0]
eps_p_36 = (1 - t_36) * unload_36.iloc[-1,0] + t_36 * unload_36.iloc[-4,0]
eps_p_45 = (1 - t_45) * unload_45.iloc[-1,0] + t_45 * unload_45.iloc[-4,0]
#
# Add intersection point to data
#
unload_09.loc[unload_09.index.max()+1] = (eps_p_09, 0.0)
unload_18.loc[unload_18.index.max()+1] = (eps_p_18, 0.0)
unload_27.loc[unload_27.index.max()+1] = (eps_p_27, 0.0)
unload_36.loc[unload_36.index.max()+1] = (eps_p_36, 0.0)
unload_45.loc[unload_45.index.max()+1] = (eps_p_45, 0.0)
#
# Separate out strain and pressure data
#
eps_09 = unload_09.iloc[:,0]
eps_18 = unload_18.iloc[:,0]
eps_27 = unload_27.iloc[:,0]
eps_36 = unload_36.iloc[:,0]
eps_45 = unload_45.iloc[:,0]
p_09 = unload_09.iloc[:,1]
p_18 = unload_18.iloc[:,1]
p_27 = unload_27.iloc[:,1]
p_36 = unload_36.iloc[:,1]
p_45 = unload_45.iloc[:,1]
eps_p = (eps_p_09, eps_p_18, eps_p_27, eps_p_36, eps_p_45, 0.0)
#
# Create data for 0% plastic strain
#
eps_p_00_data = pd.DataFrame({'TotalStrainVol' : eps_09 - eps_p_09, 'Pressure' : p_09})
eps_00 = eps_p_00_data.iloc[:,0]
p_00 = eps_p_00_data.iloc[:,1]
print(eps_00)
#
# Define functions for creating extended data in compression and tension
#
def create_extra_compression_data(eps, p, eps_max):
    x0_com = eps.values[1]
    x1_com = eps.values[0]
    y0_com = p.values[1]
    y1_com = p.values[0]
    com_strain = eps_max
    t_com_strain = (com_strain - x0_com)/(x1_com - x0_com)
    dx_com = (x1_com - x0_com)*10
    nx_com = ((com_strain - x1_com)/dx_com).astype(int)
    t_com = np.linspace(1.001, t_com_strain, nx_com)
    x_com = list(map(lambda t : (1 - t)*x0_com + t*x1_com, t_com))
    y_com = list(map(lambda t : (1 - t)*y0_com + t*y1_com, t_com))
    com_data = pd.DataFrame({'TotalStrainVol' : x_com, 'Pressure' : y_com})
    return com_data
#
def create_extra_tension_data(eps, p, eps_min):
    x0_ten = eps.values[-2]
    x1_ten = eps.values[-1]
    y0_ten = p.values[-2]
    y1_ten = p.values[-1]
    ten_strain = eps_min
    t_ten_strain = (ten_strain - x0_ten)/(x1_ten - x0_ten)
    dx_ten = (x0_ten - x1_ten)*10
    nx_ten = ((x1_ten - ten_strain)/dx_ten).astype(int)
    t_ten = np.linspace(1.001, t_ten_strain, nx_ten)
    x_ten = list(map(lambda t : (1 - t)*x0_ten + t*x1_ten, t_ten))
    y_ten = list(map(lambda t : (1 - t)*y0_ten + t*y1_ten, t_ten))
    ten_data = pd.DataFrame({'TotalStrainVol' : x_ten, 'Pressure' : y_ten})
    return ten_data
#
# Create extra data in compression and tension
#
data_00_com_extra = create_extra_compression_data(eps_00, p_00, 15)
data_09_com_extra = create_extra_compression_data(eps_09, p_09, 25)
data_18_com_extra = create_extra_compression_data(eps_18, p_18, 30)
data_27_com_extra = create_extra_compression_data(eps_27, p_27, 35)
#
data_00_ten_extra = create_extra_tension_data(eps_00, p_00, -10)
data_09_ten_extra = create_extra_tension_data(eps_09, p_09, -5)
data_18_ten_extra = create_extra_tension_data(eps_18, p_18, 0)
data_27_ten_extra = create_extra_tension_data(eps_27, p_27, 5)
data_36_ten_extra = create_extra_tension_data(eps_36, p_36, 20)
data_45_ten_extra = create_extra_tension_data(eps_45, p_45, 30)
#
# Save the input unloading data in a data frame
#
data_00_orig = pd.DataFrame({'TotalStrainVol' : eps_00, 'Pressure' : p_00})
data_09_orig = pd.DataFrame({'TotalStrainVol' : eps_09, 'Pressure' : p_09})
data_18_orig = pd.DataFrame({'TotalStrainVol' : eps_18, 'Pressure' : p_18})
data_27_orig = pd.DataFrame({'TotalStrainVol' : eps_27, 'Pressure' : p_27})
data_36_orig = pd.DataFrame({'TotalStrainVol' : eps_36, 'Pressure' : p_36})
data_45_orig = pd.DataFrame({'TotalStrainVol' : eps_45, 'Pressure' : p_45})
#
# Convert into loading form (increasing compressive strains)
#
data_00_asc = data_00_orig.sort_index(ascending=False)
data_09_asc = data_09_orig.sort_index(ascending=False)
data_18_asc = data_18_orig.sort_index(ascending=False)
data_27_asc = data_27_orig.sort_index(ascending=False)
data_36_asc = data_36_orig.sort_index(ascending=False)
data_45_asc = data_45_orig.sort_index(ascending=False)
#
# Convert the extra tensile data into compressive loading form
#
data_00_ten_asc = data_00_ten_extra.sort_index(ascending=False)
data_09_ten_asc = data_09_ten_extra.sort_index(ascending=False)
data_18_ten_asc = data_18_ten_extra.sort_index(ascending=False)
data_27_ten_asc = data_27_ten_extra.sort_index(ascending=False)
data_36_ten_asc = data_36_ten_extra.sort_index(ascending=False)
data_45_ten_asc = data_45_ten_extra.sort_index(ascending=False)
#
# Merge the data frames together
#
data_00 = data_00_ten_asc.append(data_00_asc, ignore_index = True)
data_00 = data_00.append(data_00_com_extra, ignore_index = True)
data_09 = data_09_ten_asc.append(data_09_asc, ignore_index = True)
data_09 = data_09.append(data_09_com_extra, ignore_index = True)
data_18 = data_18_ten_asc.append(data_18_asc, ignore_index = True)
data_18 = data_18.append(data_18_com_extra, ignore_index = True)
data_27 = data_27_ten_asc.append(data_27_asc, ignore_index = True)
data_27 = data_27.append(data_27_com_extra, ignore_index = True)
data_36 = data_36_ten_asc.append(data_36_asc, ignore_index = True)
data_45 = data_45_ten_asc.append(data_45_asc, ignore_index = True)
#
# Extract back into separate strain and pressure variables
#
eps_00_extra = data_00.iloc[:,0]
eps_09_extra = data_09.iloc[:,0]
eps_18_extra = data_18.iloc[:,0]
eps_27_extra = data_27.iloc[:,0]
eps_36_extra = data_36.iloc[:,0]
eps_45_extra = data_45.iloc[:,0]
p_00_extra = data_00.iloc[:,1]
p_09_extra = data_09.iloc[:,1]
p_18_extra = data_18.iloc[:,1]
p_27_extra = data_27.iloc[:,1]
p_36_extra = data_36.iloc[:,1]
p_45_extra = data_45.iloc[:,1]
#
# Set up strains for training 
#
eps_p_00 = 0
strains_00 = np.column_stack((eps_00_extra, np.repeat(eps_p_00, eps_00_extra.shape[0])))
strains_09 = np.column_stack((eps_09_extra, np.repeat(eps_p_09, eps_09_extra.shape[0])))
strains_18 = np.column_stack((eps_18_extra, np.repeat(eps_p_18, eps_18_extra.shape[0])))
strains_27 = np.column_stack((eps_27_extra, np.repeat(eps_p_27, eps_27_extra.shape[0])))
strains_36 = np.column_stack((eps_36_extra, np.repeat(eps_p_36, eps_36_extra.shape[0])))
strains_45 = np.column_stack((eps_45_extra, np.repeat(eps_p_45, eps_45_extra.shape[0])))
# 
# Collect strains and pressures together
#
strains = np.concatenate((strains_00, strains_09, strains_18, strains_27, strains_36, strains_45), axis=0)
pressures = np.concatenate((p_00_extra, p_09_extra, p_18_extra, p_27_extra, p_36_extra, p_45_extra), axis=0)
#
# Do bootstrapped data generation
#
data_00_3d = np.column_stack((strains_00, p_00_extra))
data_09_3d = np.column_stack((strains_09, p_09_extra))
data_18_3d = np.column_stack((strains_18, p_18_extra))
data_27_3d = np.column_stack((strains_27, p_27_extra))
data_36_3d = np.column_stack((strains_36, p_36_extra))
data_45_3d = np.column_stack((strains_45, p_45_extra))
data_all =  np.concatenate((data_00_3d, data_00_3d, data_00_3d, data_00_3d, 
                             data_09_3d, data_09_3d, data_09_3d, 
                             data_18_3d, data_18_3d, data_18_3d, 
                             data_27_3d, data_27_3d, 
                             data_36_3d, data_36_3d, 
                             data_45_3d), axis=0)          
#
# Shuffle the data
#
np.random.shuffle(data_all)
strains_shuffle = data_all[:,(0,1)]
pressures_shuffle = data_all[:,2]
#
# Separate into train and test sets
#
eps_train, eps_test, p_train, p_test = train_test_split(strains, pressures, 
                                                        test_size=0.4, random_state=0)
#
# Set up data scaling functions
#
strain_scaler = preprocessing.MinMaxScaler()
strain_scaler_min_max = strain_scaler.fit(strains)
scaled_strains_train = strain_scaler_min_max.transform(eps_train)
scaled_strains_test = strain_scaler_min_max.transform(eps_test)
scaled_strains = strain_scaler_min_max.transform(strains_shuffle)
#
pressure_scaler = preprocessing.MinMaxScaler()
pressure_scaler_min_max = pressure_scaler.fit(pressures.reshape(-1, 1))
scaled_pressures_train = pressure_scaler_min_max.transform(p_train.reshape(-1, 1))
scaled_pressures_test = pressure_scaler_min_max.transform(p_test.reshape(-1, 1))
scaled_pressures = pressure_scaler_min_max.transform(pressures_shuffle.reshape(-1, 1))
#
# Do SVR fit
#
curve_fitter_1_01 = SVR(kernel='rbf', C=1.0, epsilon=0.01)
fit1_01 = curve_fitter_1_01.fit(scaled_strains_train, np.ravel(scaled_pressures_train))
#
# Compute error norm
#
score1_01 = fit1_01.score(scaled_strains_test, scaled_pressures_test)
#
# Compute cross validation scores to search the parameter space
#
cv = ShuffleSplit(n_splits=10, test_size=0.5, random_state=0)
scores_1_01 = cross_val_score(curve_fitter_1_01, scaled_strains, np.ravel(scaled_pressures), cv=cv)
#
# Define function for computing predicted bulk modulus adn plotting
#
def computeAndPlotBulkElastic(svgfile, curve_fitter, C, epsilon):
    s_pressures_pred_00 = curve_fitter.predict(strain_scaler_min_max.transform(strains_00))
    s_pressures_pred_09 = curve_fitter.predict(strain_scaler_min_max.transform(strains_09))
    s_pressures_pred_18 = curve_fitter.predict(strain_scaler_min_max.transform(strains_18))
    s_pressures_pred_27 = curve_fitter.predict(strain_scaler_min_max.transform(strains_27))
    s_pressures_pred_36 = curve_fitter.predict(strain_scaler_min_max.transform(strains_36))
    s_pressures_pred_45 = curve_fitter.predict(strain_scaler_min_max.transform(strains_45)) 
    
    pressures_pred_00 = pressure_scaler.inverse_transform(s_pressures_pred_00.reshape(-1,1))
    pressures_pred_09 = pressure_scaler.inverse_transform(s_pressures_pred_09.reshape(-1,1))
    pressures_pred_18 = pressure_scaler.inverse_transform(s_pressures_pred_18.reshape(-1,1))
    pressures_pred_27 = pressure_scaler.inverse_transform(s_pressures_pred_27.reshape(-1,1))
    pressures_pred_36 = pressure_scaler.inverse_transform(s_pressures_pred_36.reshape(-1,1))
    pressures_pred_45 = pressure_scaler.inverse_transform(s_pressures_pred_45.reshape(-1,1))
    
    # Compute tangent moduli for input data
    K_00 = np.gradient(p_00_extra*1.0e6, eps_00_extra*0.01)
    K_09 = np.gradient(p_09_extra*1.0e6, eps_09_extra*0.01)
    K_18 = np.gradient(p_18_extra*1.0e6, eps_18_extra*0.01)
    K_27 = np.gradient(p_27_extra*1.0e6, eps_27_extra*0.01)
    K_36 = np.gradient(p_36_extra*1.0e6, eps_36_extra*0.01)
    K_45 = np.gradient(p_45_extra*1.0e6, eps_45_extra*0.01)
    
    # Compute tangent moduli for predicted data
    #print(pressures_pred_09.shape, strains_09.shape)
    K_pred_00 = np.gradient(np.ravel(pressures_pred_00)*1.0e6, strains_00[:,0]*0.01)
    K_pred_09 = np.gradient(np.ravel(pressures_pred_09)*1.0e6, strains_09[:,0]*0.01)
    K_pred_18 = np.gradient(np.ravel(pressures_pred_18)*1.0e6, strains_18[:,0]*0.01)
    K_pred_27 = np.gradient(np.ravel(pressures_pred_27)*1.0e6, strains_27[:,0]*0.01)
    K_pred_36 = np.gradient(np.ravel(pressures_pred_36)*1.0e6, strains_36[:,0]*0.01)
    K_pred_45 = np.gradient(np.ravel(pressures_pred_45)*1.0e6, strains_45[:,0]*0.01)
    
    # Compute error
    K_error_00 = list(map(lambda x, y: 1 if x == 0 else (x - y)/x * 100, K_00, K_pred_00))
    K_error_09 = list(map(lambda x, y: 1 if x == 0 else (x - y)/x * 100, K_09, K_pred_09))
    K_error_18 = list(map(lambda x, y: 1 if x == 0 else (x - y)/x * 100, K_18, K_pred_18))
    K_error_27 = list(map(lambda x, y: 1 if x == 0 else (x - y)/x * 100, K_27, K_pred_27))
    K_error_36 = list(map(lambda x, y: 1 if x == 0 else (x - y)/x * 100, K_36, K_pred_36))
    K_error_45 = list(map(lambda x, y: 1 if x == 0 else (x - y)/x * 100, K_45, K_pred_45))
    
    plot_label = "SVR (C=" + str(C) + ", $\epsilon$=" + str(epsilon) + ")"
    fig = plt.figure(figsize=(14,6))
    ax = fig.add_subplot(121)
    plt.plot(eps_00_extra - eps_p_00, K_00*1.0e-9, 'C5', label=labels[5])
    plt.plot(eps_09_extra - eps_p_09, K_09*1.0e-9, 'k-', label=labels[0])
    plt.plot(eps_18_extra - eps_p_18, K_18*1.0e-9, 'C0', label=labels[1])
    plt.plot(eps_27_extra - eps_p_27, K_27*1.0e-9, 'C3', label=labels[2])
    plt.plot(eps_36_extra - eps_p_36, K_36*1.0e-9, 'C1', label=labels[3])
    plt.plot(eps_45_extra - eps_p_45, K_45*1.0e-9, 'C4', label=labels[4])
    plt.plot(strains_00[:,0] - eps_p_00, K_pred_00*1.0e-9, 'C5--', label=plot_label, linewidth=2)
    plt.plot(strains_09[:,0] - eps_p_09, K_pred_09*1.0e-9, 'k--', linewidth=2)
    plt.plot(strains_18[:,0] - eps_p_18, K_pred_18*1.0e-9, 'C0--', linewidth=2)
    plt.plot(strains_27[:,0] - eps_p_27, K_pred_27*1.0e-9, 'C3--', linewidth=2)
    plt.plot(strains_36[:,0] - eps_p_36, K_pred_36*1.0e-9, 'C1--', linewidth=2)
    plt.plot(strains_45[:,0] - eps_p_45, K_pred_45*1.0e-9, 'C4--', linewidth=2)
    
    #plt.axis([-10, 45, -10, 50])
    plt.axis([0, 10, 0, 50])
    plt.xlabel('Elastic volumetric strain (%)', fontsize=16)
    plt.ylabel('Tangent buk modulus (GPa)', fontsize=16)
    ax.legend(loc='best', fontsize=10)  
    
    ax = fig.add_subplot(122)
    plt.plot(eps_00_extra - eps_p_00, K_error_00, 'C5', label=labels[5])
    plt.plot(eps_09_extra - eps_p_09, K_error_09, 'k', label=labels[0])
    plt.plot(eps_18_extra - eps_p_18, K_error_18, 'C0', label=labels[1])
    plt.plot(eps_27_extra - eps_p_27, K_error_27, 'C3', label=labels[2])
    plt.plot(eps_36_extra - eps_p_36, K_error_36, 'C1', label=labels[3])
    plt.plot(eps_45_extra - eps_p_45, K_error_45, 'C4', label=labels[4])
    #plt.axis([0, 14, -3, 4])
    plt.xlabel('Elastic volumetric strain (%)', fontsize=16)
    plt.ylabel('Error in predicted modulus (%)', fontsize=16)
    ax.legend(loc='best', fontsize=10) 
    
    #fig.savefig(svgfile)
#
# Create plot of the fit
#
computeAndPlotBulkElastic('Fox_DrySand_BulkModulus_TotalElasticStrain_Tension_SVR_1_01_scaled_cv.svg', curve_fitter_1_01, 1, 0.01)
#
# Extract the SVR parameters for the fit
#
print(curve_fitter_1_01.support_.shape, curve_fitter_1_01.support_vectors_.shape,
      curve_fitter_1_01.dual_coef_.shape, curve_fitter_1_01.fit_status_,
      curve_fitter_1_01.intercept_)
#
# Compute gamma
#
gamma = 1/(2*scaled_strains_train.var())
#
# Get the scaling parameters
#
eps_scale = strain_scaler.scale_
eps_min = strain_scaler.data_min_
eps_max = strain_scaler.data_max_
p_scale = pressure_scaler.scale_
p_min = pressure_scaler.data_min_
p_max = pressure_scaler.data_max_
#
# Save as JSON
#
class NumpyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return super(NumpyArrayEncoder, self).default(obj)
        
# Save the scaling parameters, kernel parameters, support vectors, duals, and intercept
def save_svr_json(json_file, X_var, y_var, X_conv, y_conv, X_scaler, y_scaler, X_train_scaled, fitter):
    X_scale = X_scaler.scale_
    X_min = X_scaler.data_min_
    X_max = X_scaler.data_max_
    y_scale = y_scaler.scale_
    y_min = y_scaler.data_min_
    y_max = y_scaler.data_max_
    d = X_train_scaled.shape[1]
    sigma_sq = X_train_scaled.var()
    gamma = 1/(d*sigma_sq)
    X_sup_vec = fitter.support_vectors_
    X_dual = fitter.dual_coef_
    X_intercept = fitter.intercept_
    json_data = {}
    json_data['X_var'] = X_var
    json_data['y_var'] = y_var
    json_data['X_conversion_factor'] = X_conv
    json_data['y_conversion_factor'] = y_conv
    json_data['X_scale'] = X_scale
    json_data['X_min'] = X_min
    json_data['X_max'] = X_max
    json_data['y_scale'] = y_scale
    json_data['y_min'] = y_min
    json_data['y_max'] = y_max
    json_data['gamma'] = gamma
    json_data['support_vectors'] = X_sup_vec
    json_data['dual_coeffs'] = X_dual
    json_data['intercept'] = X_intercept
    # Set numpy's printoptions to display all the data with max precision
    np.set_printoptions(threshold=np.inf,
                        linewidth=sys.maxsize,
                        suppress=True,
                        nanstr='0.0',
                        infstr='0.0', 
                        precision=np.finfo(np.longdouble).precision)     
    with open(json_file, 'w') as outfile:
        json.dump(json_data, outfile, cls=NumpyEncoder, indent=2)
#
save_svr_json('ARL_Sand_SVR_fit_10_001.json', 
              ['Total volumetric strain (%)', 'Plastic volumetric strain (%)'], 
              pressure_scaler, scaled_strains_train, curve_fitter_10_001)
\end{lstlisting}


\subsection{Neural-network model for the bulk modulus}
Instead of a tabular or a support vector model, the \Textsfc{tabular plasticity} model can also use
a neural network model.  This model can be invoked using:
\begin{lstlisting}[language=XML]
<constitutive_model type="tabular_plasticity_cap">
  <elastic_moduli_model type="neural_net_bulk">
    <filename>mlp_regression.h5</filename>
    <mean_elastic_strain>0.0</mean_elastic_strain>
    <mean_plastic_strain>0.5</mean_plastic_strain>
    <std_dev_elastic_strain>1.0</std_dev_elastic_strain>
    <std_dev_plastic_strain>0.5</std_dev_plastic_strain>
    <mean_bulk_modulus>0.0</mean_bulk_modulus>
    <std_dev_bulk_modulus>1.0e6</std_dev_bulk_modulus>
    <G0>3500</G0>
    <nu>0.189</nu>
  </elastic_moduli_model>
  ...
</constitutive_model>
\end{lstlisting}
The input file is required to contains a HDF5 representation of the fitted neural network 
produced by \Textsfc{Tensorflow 2.0}.  The mean and standard deviations are required as inputs
with the assumption that the input data were fitted after scaling the data such that the
mean was zero and the standard deviation 1.

An example of fitting a neural network is given below.
\begin{lstlisting}[language=Python]
# Load the required packages
import numpy as np
import pandas as pd
import PolylineIntersection as pl
from keras.models import Sequential
from keras.layers import Dense
from keras.utils import plot_model
import matplotlib.pyplot as plt
from sklearn import preprocessing
from keras.models import load_model
import h5py

# Load the CSV data into Pandas dataframes
hydrostat = pd.read_csv("./DrySand_Hydrostat.csv", header=0, skiprows=7)
data_09 = pd.read_csv("./DrySand_LoadUnload_09.csv", header=0, skiprows=4)
data_18 = pd.read_csv("./DrySand_LoadUnload_18.csv", header=0, skiprows=4)
data_27 = pd.read_csv("./DrySand_LoadUnload_27.csv", header=0, skiprows=4)
data_36 = pd.read_csv("./DrySand_LoadUnload_36.csv", header=0, skiprows=4)
data_45 = pd.read_csv("./DrySand_LoadUnload_45.csv", header=0, skiprows=4)

# Rename the columns of each dataframe
column_names = ["TotalStrainVol", "Pressure", "", "", "", ""]
hydrostat.columns = column_names
data_09.columns = column_names
data_18.columns = column_names
data_27.columns = column_names
data_36.columns = column_names
data_45.columns = column_names

# Convert percent into strain, MPa to Pa
strain_fac = 0.01
pressure_fac = 1.0e6
hydrostat.TotalStrainVol *= strain_fac
hydrostat.Pressure *= pressure_fac
data_09.TotalStrainVol *= strain_fac
data_09.Pressure *= pressure_fac
data_18.TotalStrainVol *= strain_fac
data_18.Pressure *= pressure_fac
data_27.TotalStrainVol *= strain_fac
data_27.Pressure *= pressure_fac
data_36.TotalStrainVol *= strain_fac
data_36.Pressure *= pressure_fac
data_45.TotalStrainVol *= strain_fac
data_45.Pressure *= pressure_fac

# Find the point at which unloading begins
p_max_09 = max(data_09.Pressure)
p_max_index_09 = data_09.Pressure.values.tolist().index(p_max_09)
p_max_18 = max(data_18.Pressure)
p_max_index_18 = data_18.Pressure.values.tolist().index(p_max_18)
p_max_27 = max(data_27.Pressure)
p_max_index_27 = data_27.Pressure.values.tolist().index(p_max_27)
p_max_36 = max(data_36.Pressure)
p_max_index_36 = data_36.Pressure.values.tolist().index(p_max_36)
p_max_45 = max(data_45.Pressure)
p_max_index_45 = data_45.Pressure.values.tolist().index(p_max_45)
p_max_index_00 = (np.abs(p_max_09 - hydrostat.Pressure.values)).argmin()
p_max_00 = hydrostat.Pressure.values[p_max_index_00]

# Create separate dataframes for the unload data
data_09_unload = data_09[p_max_index_09:].copy()
data_18_unload = data_18[p_max_index_18:].copy()
data_27_unload = data_27[p_max_index_27:].copy()
data_36_unload = data_36[p_max_index_36:].copy()
data_45_unload = data_45[p_max_index_45:].copy()

# Find plastic strains by intersecting the unload data with the pressure axis
pressure_axis = ((-1, 0),(1, 0))
poly_09_unload = list(data_09_unload[['TotalStrainVol', 'Pressure']].apply(tuple, axis=1))
line_09_unload = (poly_09_unload[-1], poly_09_unload[-2])
plastic_strain_09 = pl.line_intersection(pressure_axis, line_09_unload)[0]
poly_18_unload = list(data_18_unload[['TotalStrainVol', 'Pressure']].apply(tuple, axis=1))
line_18_unload = (poly_18_unload[-1], poly_18_unload[-2])
plastic_strain_18 = pl.line_intersection(pressure_axis, line_18_unload)[0]
poly_27_unload = list(data_27_unload[['TotalStrainVol', 'Pressure']].apply(tuple, axis=1))
line_27_unload = (poly_27_unload[-1], poly_27_unload[-2])
plastic_strain_27 = pl.line_intersection(pressure_axis, line_27_unload)[0]
poly_36_unload = list(data_36_unload[['TotalStrainVol', 'Pressure']].apply(tuple, axis=1))
line_36_unload = (poly_36_unload[-1], poly_36_unload[-2])
plastic_strain_36 = pl.line_intersection(pressure_axis, line_36_unload)[0]
poly_45_unload = list(data_45_unload[['TotalStrainVol', 'Pressure']].apply(tuple, axis=1))
line_45_unload = (poly_45_unload[-1], poly_45_unload[-2])
plastic_strain_45 = pl.line_intersection(pressure_axis, line_45_unload)[0]
print(plastic_strain_09, plastic_strain_18, plastic_strain_27, plastic_strain_36, plastic_strain_45)

# Reverse the order of the unload data to create elastic loading curves
data_00_load = hydrostat[:p_max_index_00]
data_09_load = data_09_unload.sort_index(ascending=False)
data_18_load = data_18_unload.sort_index(ascending=False)
data_27_load = data_27_unload.sort_index(ascending=False)
data_36_load = data_36_unload.sort_index(ascending=False)
data_45_load = data_45_unload.sort_index(ascending=False)

# Simplify the loading dataframes and remove duplicates (if any)
data_00_all = data_00_load[['TotalStrainVol', 'Pressure']].copy().drop_duplicates()
data_09_all = data_09_load[['TotalStrainVol', 'Pressure']].copy().drop_duplicates()
data_18_all = data_18_load[['TotalStrainVol', 'Pressure']].copy().drop_duplicates()
data_27_all = data_27_load[['TotalStrainVol', 'Pressure']].copy().drop_duplicates()
data_36_all = data_36_load[['TotalStrainVol', 'Pressure']].copy().drop_duplicates()
data_45_all = data_45_load[['TotalStrainVol', 'Pressure']].copy().drop_duplicates()

# Compute max strain and pressure
total_strain_max = data_45_all['TotalStrainVol'].max()
pressure_max = data_45_all['Pressure'].max()

# Rename strain variables
eps_p_09 = plastic_strain_09
eps_09 = data_09_all['TotalStrainVol'].values
eps_p_18 = plastic_strain_18
eps_18 = data_18_all['TotalStrainVol'].values
eps_p_27 = plastic_strain_27
eps_27 = data_27_all['TotalStrainVol'].values
eps_p_36 = plastic_strain_36
eps_36 = data_36_all['TotalStrainVol'].values
eps_p_45 = plastic_strain_45
eps_45 = data_45_all['TotalStrainVol'].values
eps_00 = np.linspace(0, total_strain_max)
eps_30 = np.linspace(0, total_strain_max)
eps_p_00 = 0
eps_p_30 = 0.5*(eps_p_27 + eps_p_36)

# Rename pressure variables
p_09 = data_09_all['Pressure'].values
p_18 = data_18_all['Pressure'].values
p_27 = data_27_all['Pressure'].values
p_36 = data_36_all['Pressure'].values
p_45 = data_45_all['Pressure'].values

# Scale the data
def scaled(x, min_x, max_x):
    return (x - min_x)/(max_x - min_x)

# Unscale the data
def unscaled(x, min_x, max_x):
    return min_x + x * (max_x - min_x)

# Convert strains to scaled values
eps_09_scaled = scaled(eps_09, 0, total_strain_max)
eps_18_scaled = scaled(eps_18, 0, total_strain_max)
eps_27_scaled = scaled(eps_27, 0, total_strain_max)
eps_36_scaled = scaled(eps_36, 0, total_strain_max)
eps_45_scaled = scaled(eps_45, 0, total_strain_max)
eps_00_scaled = scaled(eps_00, 0, total_strain_max)
eps_30_scaled = scaled(eps_30, 0, total_strain_max)
eps_p_09_scaled = scaled(eps_p_09, 0, total_strain_max)
eps_p_18_scaled = scaled(eps_p_18, 0, total_strain_max)
eps_p_27_scaled = scaled(eps_p_27, 0, total_strain_max)
eps_p_36_scaled = scaled(eps_p_36, 0, total_strain_max)
eps_p_45_scaled = scaled(eps_p_45, 0, total_strain_max)
eps_p_00_scaled = scaled(eps_p_00, 0, total_strain_max)
eps_p_30_scaled = scaled(eps_p_30, 0, total_strain_max)

# Convert pressures to scaled values
p_09_scaled = scaled(p_09, 0, 1.0e6)
p_18_scaled = scaled(p_18, 0, 1.0e6)
p_27_scaled = scaled(p_27, 0, 1.0e6)
p_36_scaled = scaled(p_36, 0, 1.0e6)
p_45_scaled = scaled(p_45, 0, 1.0e6)

# Set up data frames for regression
strains_09_scaled = np.column_stack((eps_09_scaled, np.repeat(eps_p_09_scaled, eps_09_scaled.shape[0])))
strains_18_scaled = np.column_stack((eps_18_scaled, np.repeat(eps_p_18_scaled, eps_18_scaled.shape[0])))
strains_27_scaled = np.column_stack((eps_27_scaled, np.repeat(eps_p_27_scaled, eps_27_scaled.shape[0])))
strains_36_scaled = np.column_stack((eps_36_scaled, np.repeat(eps_p_36_scaled, eps_36_scaled.shape[0])))
strains_45_scaled = np.column_stack((eps_45_scaled, np.repeat(eps_p_45_scaled, eps_45_scaled.shape[0])))
strains_30_scaled = np.column_stack((eps_30_scaled, np.repeat(eps_p_30_scaled, eps_30_scaled.shape[0])))
strains_00_scaled = np.column_stack((eps_00_scaled, np.repeat(eps_p_00_scaled, eps_00_scaled.shape[0])))
strains_scaled = np.concatenate((strains_09_scaled, strains_18_scaled, strains_27_scaled, strains_36_scaled, strains_45_scaled), axis=0)
pressures_scaled = np.concatenate((p_09_scaled, p_18_scaled, p_27_scaled, p_36_scaled, p_45_scaled), axis=0)

data_09_scaled = np.column_stack((strains_09_scaled, p_09_scaled))
data_18_scaled = np.column_stack((strains_18_scaled, p_18_scaled))
data_27_scaled = np.column_stack((strains_27_scaled, p_27_scaled))
data_36_scaled = np.column_stack((strains_36_scaled, p_36_scaled))
data_45_scaled = np.column_stack((strains_45_scaled, p_45_scaled))

# Bootstrapping step
data_all_scaled =  np.concatenate((data_09_scaled, data_18_scaled, data_27_scaled, data_36_scaled, data_45_scaled, 
                            data_09_scaled, data_09_scaled, data_09_scaled, data_09_scaled,
                            data_18_scaled, data_18_scaled, data_18_scaled, data_18_scaled,
                            data_27_scaled, data_27_scaled, data_27_scaled, data_27_scaled,
                            data_36_scaled, data_36_scaled, data_36_scaled, data_36_scaled,
                            data_45_scaled, data_45_scaled, data_45_scaled, data_45_scaled), axis=0)    
#np.random.seed(12345)
np.random.shuffle(data_all_scaled)
strains_shuffle = data_all_scaled[:,(0,1)]
pressures_shuffle = data_all_scaled[:,2]

# Set up the neural network
def baseline2D_model():
    model = Sequential()
    model.add(Dense(64, input_dim=2, kernel_initializer='normal', activation='sigmoid'))
    model.add(Dense(32, kernel_initializer='normal', activation='sigmoid'))
    model.add(Dense(32, kernel_initializer='normal', activation='relu'))
    model.add(Dense(1, kernel_initializer='normal'))
    model.compile(loss='mean_squared_error', optimizer='adam')
    return model

model2D = baseline2D_model()

# Fit the model
model2D.fit(strains_shuffle, pressures_shuffle, batch_size=192, epochs=3000, verbose=1, validation_split=0.2, shuffle=True)

# Compute predicted values
pressures_pred_09_scaled = model2D.predict(strains_09_scaled)
pressures_pred_18_scaled = model2D.predict(strains_18_scaled)
pressures_pred_27_scaled = model2D.predict(strains_27_scaled)
pressures_pred_36_scaled = model2D.predict(strains_36_scaled)
pressures_pred_45_scaled = model2D.predict(strains_45_scaled)
pressures_pred_30_scaled = model2D.predict(strains_30_scaled)
pressures_pred_00_scaled = model2D.predict(strains_00_scaled)

# Unscale the data
strains_09 = unscaled(strains_09_scaled, 0, total_strain_max)
strains_18 = unscaled(strains_18_scaled, 0, total_strain_max)
strains_27 = unscaled(strains_27_scaled, 0, total_strain_max)
strains_36 = unscaled(strains_36_scaled, 0, total_strain_max)
strains_45 = unscaled(strains_45_scaled, 0, total_strain_max)
strains_30 = unscaled(strains_30_scaled, 0, total_strain_max)
strains_00 = unscaled(strains_00_scaled, 0, total_strain_max)

pressure_max = 1.0e6
pressures_pred_09 = unscaled(pressures_pred_09_scaled, 0, pressure_max)
pressures_pred_18 = unscaled(pressures_pred_18_scaled, 0, pressure_max)
pressures_pred_27 = unscaled(pressures_pred_27_scaled, 0, pressure_max)
pressures_pred_36 = unscaled(pressures_pred_36_scaled, 0, pressure_max)
pressures_pred_45 = unscaled(pressures_pred_45_scaled, 0, pressure_max)
pressures_pred_30 = unscaled(pressures_pred_30_scaled, 0, pressure_max)
pressures_pred_00 = unscaled(pressures_pred_00_scaled, 0, pressure_max)

# Set up rescale function for plotting
def rescale(data, VARIABLE_TYPE):
    result = {
        "e" : lambda data : data*100.0,
        "p" : lambda data : data*1.0e-6
    }[VARIABLE_TYPE](data)
    return result

# Plot the original vs predicted data
lab_09 = str("Plastic strain = {0:.3f}".format(eps_p_09))
lab_18 = str("Plastic strain = {0:.3f}".format(eps_p_18))
lab_27 = str("Plastic strain = {0:.3f}".format(eps_p_27))
lab_36 = str("Plastic strain = {0:.3f}".format(eps_p_36))
lab_45 = str("Plastic strain = {0:.3f}".format(eps_p_45))
lab_00 = str("MLP: Plastic strain = {0:.3f}".format(eps_p_00))
lab_30 = str("MLP: Plastic strain = {0:.3f}".format(eps_p_30))
type(lab_09)

fig = plt.figure(figsize=(6,6))
ax = fig.add_subplot(111)
plt.plot(rescale(eps_09, "e"), rescale(p_09, "p"), 'k--', label=lab_09)
plt.plot(rescale(eps_18, "e"), rescale(p_18, "p"), 'C0--', label=lab_18)
plt.plot(rescale(eps_27, "e"), rescale(p_27, "p"), 'C3--', label=lab_27)
plt.plot(rescale(eps_36, "e"), rescale(p_36, "p"), 'C1--', label=lab_36)
plt.plot(rescale(eps_45, "e"), rescale(p_45, "p"), 'C4--', label=lab_45)
plt.plot(rescale(strains_09[:,0], "e"), rescale(pressures_pred_09, "p"), 'k', linewidth=2, label='MLP')
plt.plot(rescale(strains_18[:,0], "e"), rescale(pressures_pred_18, "p"), 'C0', linewidth=2)
plt.plot(rescale(strains_27[:,0], "e"), rescale(pressures_pred_27, "p"), 'C3', linewidth=2)
plt.plot(rescale(strains_36[:,0], "e"), rescale(pressures_pred_36, "p"), 'C1', linewidth=2)
plt.plot(rescale(strains_45[:,0], "e"), rescale(pressures_pred_45, "p"), 'C4', linewidth=2)
plt.plot(rescale(strains_30[:,0], "e"), rescale(pressures_pred_30, "p"), 'C7', linewidth=2, label=lab_30)
plt.plot(rescale(strains_00[:,0], "e"), rescale(pressures_pred_00, "p"), 'C8', linewidth=2, label=lab_00)
plt.axis([0, 50, 0, 3000])
plt.xlabel('Total volumetric strain (%)', fontsize=16)
plt.ylabel('Pressure (MPa)', fontsize=16)
ax.legend(loc='best', fontsize=14)
fig.savefig('Fox_DrySand_ElasticStrain_MLP_Total.svg')

# Save the model and fit in binary HDF5 format for reading into Vaango
model2D.save('mlp_regression_keras_total_scaled.h5')
\end{lstlisting}
